```dockerfile
                > 포드를 사용하는 다른 오브젝트들
    
    포드를 사용하는 다른 상위 오브젝트에서는 포드의 기능을 그대로 사용할 수 있기 때문에 지금까지 쿠버네티스의 기능을 사용할 때는 대부분 포드 기준으로 보면 
    됐다. 오브젝트 내에서 포드를 사용할 경우 YAML 파일 등에서 포드 템플릿을 이용해 포드의 기능을 정의할 수 있기 때문이다. 대표적인 예시로 디플로이먼트에서
    포드를 사용했던 것을 떠올려보자. 디플로이먼트에서는 spec.template 항목을 통해 사용할 포드의 기능을 정의했다. 
    ...
        matchLabels:
            app: webserver
        template:
            metadata:
                name: my-webserver
                labels:
                    app: webserver
        spec:
            containers:
            - name: my-webserver
              image: alicek106/rr-test:echo-hostname
              ports:
              - cotainerPort: 80
              
      ==> 디플로이먼트에서 포드를 정의하는 템플릿 
  
    이처럼 포드를 사용하는 상위 오브젝트에는 디플로이먼트 외에도 몇 가지가 더 있다. 
    
    
    
            > 1. 잡(Jobs)
    잡(Job)은 특정 동작을 수행하고 종료해야하는 작업을 위한 오브젝트이다. 포드를 생성해 원하는 동적을 수행한다는 점에서는 디플로이먼트와 같지만, 잡에서 
    원하는 최종 상태는 '특정 개수의 포드가 실행 중인 것'이 아닌 '포드가 실행되어 정상적으로 종료되는 것'이라는 점에서 차이가 있다. 따라서 잡에서 포드의
    컨테이너가 종료 코드로서 0을 반환해 Completed 상태가 되는 것을 목표로 한다. 
    
    예를 들어, 포드에서 Hello, World만을 출력하고 종료되는 간단한 잡을 생성해보자. 
```
job-hello-world.yaml
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: job-hello-world
spec:
  template:
    spec: 
      restartPolicy: Never
      containers:
      - image: busybox
        args: ['sh', '-c' 'echo hello, world && exit 0']
        name: job-hello-world
```
```dockerfile
    kind가 Job으로 설정됐다는 점을 제외하면 위의 YAML 파일은 단일 포드를 정의하는 YAML 파일과 다른 점은 거의 없어 보인다. 단, 앞서 설명한 것처럼
    잡의 포드가 최종적으로 도달해야하는 상태는 Running이 아니라 completed이기 때문에 포드의 restartPolicy를 명시적으로 Never 또는 OnFailure로
    지정해 줘야 한다.
    위 YAML로 잡을 생성한 뒤, 잡과 포드의 목록을 확인해보자. 
    
            'kubectl apply -f job-hello-world.yaml'
            'kubectl get pods'
            'kubectl get jobs'
        
    생성된 포드는 Hello, world만 출력하고 종료되기 때문에 곧바로 Completed가 됐으며, 잡의 COMPLETIONS 항목에서는 1/1이라는 문구로 1개의 포드가 
    정상적으로 수행됐음을 나타내고 있다. 
    
    잡에서 생성된 포드는 항상 실행 중인 것을 목표로 하지 않기 때문에, 잡 오브젝트를 어디에서 사용해야 할지 의문이 들 수 있다. 사용자의 요청을 처리하는 서버와 
    같은 애플리케이션의 관점이 아닌, 한 번 수행하고 종료되는 배치(Batch) 작업을 위한 관점에서 생각해보면 잡의 쓰임새를 쉽게 이해할 수 있다. 프레임을 
    렌더링하거나 파일을 인코딩하는 작업처럼 데이터를 가공해야 하는 배치 워크로드를 쿠버네티스에서 수행해야 한다고 생각해보자. 이러한 배치 워크로드의 애플리케이션
    은 항상 실행 중일 필요가 없으며, 원하는 동작을 수행한 뒤 종료되기만 하면 그것만으로 충분하다. 이러한 상황에서는 디플로이먼트보다는 잡을 사용하면 배치
    워크로드를 조금 더 명확히 정의해 사용할 수 있다.
    
    단, 잡은 동시성을 엄격하게 보장해야 하는 병렬 처리를 위해 사용하는 것이 아니라는 점을 알아둬야 한다. 또한 잡의 실패하면 포드가 restartPolicy에 따라 
    재시작될 수도 있어서 잡이 처리하는 작업은 멱등성을 가지는 것이 좋다. 
    
        {
            쿠버네티스의 공식 문서에서는 YAML 템플릿을 이용해서 동일한 잡을 여러 개 생성하거나 Message Queue나 Redis에 작업 큐를 저장해 준 뒤
            잡이 작업 큐를 꺼내와 처리하도록 하는 패턴 등을 설명하고 있다. 
        }
        
        
            > 잡의 세부 옵션
    위 예시에서는 잡에서 포드를 하나만 생성했으며, 포드가 한 번만에 설공했기 때문에 별도의 옵션을 설정할 필요가 없었다. 하지만 실제로 배치 워크로드에서 잡을
    사용하려면 다양한 옵션을 함께 사용해야만 효율적으로 잡업을 끝낼 수 있다. 잡에서 자주 사용되는 세부 옵션은 아래와 같다.
    
        - spec.completions: 잡이 성공했다고 여겨지려면 몇 개의 포드가 성공해야하는지 (정상적으로 종료돼야 하는지) 설정한다. 기본값은 1이다.
        - spec.parallelism: 동시에 생성될 포드의 개수를 설정한다. 기본값은 1이다.
        
    spec.completions는 해당 잡이 성공했다고 여겨지려면 포드가 총 몇 개 있어야하는지를 의미한다. 기본적으로 1로 설정돼 있어서 1개의 포드가 정상적으로
    종료되면 잡 또한 성공했다고 간주한다. 만약 3으로 설정한다면? 
```
job-completion.yaml
```yaml
    ... 
      name: job-completions
    spec: 
      completions: 3
      teamplate:
    ...
```
````dockerfile
        'kubectl apply -f job-completions.yaml'
        'kubectl get pods -w'
        
                NAME           READY       STATUS          RESTARTS    AGE 
        job-completions-7fs99   0/1   ContainerCreating        0        3s
        job-completions-7fs99   0/1      Completed             0        12s 
        job-completions-mv9d7   0/1      Pending               0        0s 
        job-completions-mv9d7   0/1      Pending               0        0s 
        job-completions-mv9d7   0/1   ContainerCreating        0        0s 
        job-completions-mv9d7   0/1      Completed             0        9s 
        job-completions-lx2sh   0/1      Pending               0        0s 
        job-completions-lx2sh   0/1      Pending               0        0s 
        job-completions-lx2sh   0/1   ContainerCreating        0        0s 
        job-completions-lx2sh   0/1      Completed             0        7s
        
        포드가 순차적으로 하나씩 생성됐고, 포드가 Completed 상태가 되자마자 바로 다음 포드가 실행됐다. job-completion이라는 잡의 입장에서는 3개의
        포드가 정상적으로 종료돼야만 잡이 성공적으로 수행된 것으로 간주하기 때문에 포드르 한 개씩 3번 생성한 것이다. 그리고 결국 3개의 포드가 하나씩 
        정상적으로 종료되어 completions의 횟수를 채웠기 때문에 잡은 최종적으로 성공한 것으로 여겨진다.
        
                'kubectl get job job-completions -o yaml | grep type'
                'kubectl get jobs'
                
            {
                만약 잡의 포드가 실패한다면 restartPolicy에 따라 포드가 다시 재시작 되거나(OnFailure), 새로운 포드를 다시 생성해 똑같은 작업을
                다시 시도한다.(Never) 포드가 실패하면 기본적으로는 최대 6번을 다시 시도 하지만, 최대 재시도 횟수는 spec.backoffLimit 값에 
                별도로 설정할 수 있다.
            }
            
        이때 3개의 포드를 한꺼번에 생성하지 않고 순차적으로 생성하냐면 이는 잡이 실행할 포드의 개수를 지정하는 spec.parallelism의 값이 기본적으로 1로 
        설정돼 있기 떄문이다. 잡에서 한 번에 포드를 여러 개 생성하여 실행하고 싶다면 parallelism의 값을 적절히 설정하면 된다.  그렇다면 이번에는 
        completion 없이 parallism 값만 3설정해 잡을 실행해보자.
````
job-paralleism.yaml
```yaml
  ...
    name: job-parallelism
  spec:
      parallelism
```
```dockerfile
            'kubectl apply -f job-parallelism.yaml'
            'kubectl get pods'
            
    이번에는 한 번에 3개의 포드가 동시에 생성되고 있음을 확인할 수 있다. spec.completion과 spec.parallelism을 함께 사용하면 잡의 수행 속도를
    적절히 조정할 수 있다. 예를 들어 completion을 8, parallelism을 2로 설정했다고 가정해보자.
```
job-comple-parallel.yaml
```yaml
  ...
    name: job-comple-parallel
  spec:
    completions: 8
    parallelism: 2

```
```dockerfile
    이때 전체적으로는 8개의 포드가 정상적으로 수행돼야만 잡이 성공했다고 여겨지지만, 특정 순간에 동시에 실행될 수 있는 포드의 개수는 2개로 제한돼 있다.
    따라서 포드가 계속해서 생성되어 2개씩 실행될 것이고, 8번째 포드가 정상적으로 종료되는 순간 잡이 성공했다고 간주하기 때문에 포드를 더 이상 생성하지 
    않을 것이다. 
    
        {
            잡의 특정 포드에서 작업이 진행되지 않고 막혀 있는 경우, 잡은 성공이나 실패도 아닌 상태로 오랜 시간 정체되어 있을 것이다. 이러한 상황을 
            방지하기 위해 포드가 실행될 수 있는 최대 시간을 spec.activeDeadlineSeconds 옵션으로 제한할 수 있다. 포드가 이 옵션에 설정된 값보다 
            더 오래 실행될 경우 포드는 강제로 종료되며, 잡은 실패한 상태로 여겨진다. 
        }
        
        
        
            > 크론잡(CronJobs)으로 잡을 주기적으로 실행하기
    크론잡(CronJob)은 잡을 주기적으로 실행하는 쿠버네티스 오브젝트이다. 크론잡을 사용하면 특정 시간 간격으로 잡을 반복적으로 실행할 수 있기 때문에 데이터 
    백업이나 이메일 전송 등의 용도로 사용하기 적합하다. 
    
    크론잡은 리눅스에서 흔히 사요하는 크론(Cron)의 스케쥴 방법을 그대로 사용하기 때문에 크론의 사용방법을 알고 있다면 수월하게 사용할 수 있다.
    1분마다 잡을 실행하는 간단한 크론잡을 생성하기 위해서 아래의 YAML을 작성해보자.
```
cronjob-example.yaml
```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cronJob-example
spec:
  schedule: "*/1 * * * *"       #Job의 실행 주기 
  jobTemplate:                  #실행될 Job의 설정 내용(spec)
    spec:
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: cronjob-example
            image: busybox
            args: ['sh', '-c', 'date']
```
```dockerfile
    먼저 schedule에서는 1분에 한 번씩 실행하라는 의미의 타임 스케쥴을 설정했다. jobTemplate에서는 이전에 잡을 정의할 때 사용했던 것과 동일하게 잡의 
    spec을 그대로 입력하면 된다. 이는 schedule에 설정된 주기마다 jobTemplate의 설정값을 갖는 잡을 실행한다는 의미이다. 따라서 이 YAML 파일로 
    크론잡을 생성하면 1분마다 잡이 생성될 것이다. 
    
            'kubectl apply -f cronjob-example.yaml'
            'kubectl get jobs'
        {
            기본적으로 성공한 잡의 기록은 3개까지, 실패한 잡의 기록은 최대 1개까지만 기록하도록 설정돼 있다. 이 값은 YAML 파일에서 각각 
            spec.successfulJobsHistoryLimit 및 spec.failedJobsHistoryLimit 값을 설정함으로써 변경할 수 있다. 
        }
        
    
                > 2. 데몬셋(DaemonSets)
    데몬셋(DaemonSets)은 쿠버네티스의모든 노드에 동일한 포드를 하나씩 생성하는 오브젝트이다. 데몬셋은 로깅, 모니터링, 네트워킹 등을 위한 에이전트를 각 
    노드에 생성해야할 때 유용하게 사용할 수 있다.
    
    예를 들어, 쿠버네티스 네트워킹을 위한 kube-proxy 컴포넌트나 calico 등의 네트워크 플러그인은 kube-system 네임스페이스에서 데몬셋으로 실행되고 있다.
    calico나 kube-proxy 포드는 쿠버네티스의 오버레이 네트워크를 구성할 때 필수적인 요소이기 때문에 기본적으로 모든 노드에서 에이전트처럼 실행된다.
    
                'kubectl get daemonsets -n kube-system'
    데몬셋은 다른 오브젝트들보다 비교적 간단하기 때문에 어렵지 않게 사용할 수 있다. 모든 노드에 동일한 포드를 하나씩 배치하는 간단한 데몬셋을 생성해 보자. 
```
daemonset-example.yaml
```yaml
apiVersion: appgs/v1
kind: DaemonSet                             # [1]
metadata:
  name: daemonset-example
spec:
  selector:
    matchLabels:
      name: my-daemonset-example            # [2.1] 포드를 생성하기 위한 셀렉터 설정
  template:
    metadata:                               # [2.2] 포드 라벨 설정
      labels:
        name: my-daemonset-example
  spec:
    tolerations:                            # [3] 마스터 노드에도 포드를 생성
    - key: node-role.kubernetes.io/master
      effect: NoSchedule
    conatiners:
    - name: daemonset-example
      image: busybox                        # 테스트를 위해 busybox 이미지 사용
      args: ['tail', '-f', '/dev/null']
        resources:                          # [4] 자원 할당량을 제한
          limits:
            cpu: 100m
            memory: 200Mi
```
```dockerfile
    [1]: 가장 먼저, kind: DaemonSet이라고 명시함으로써 데몬셋을 사용할 것임을 선언한다. 
    [2]: 데몬셋은 디플로이먼트처럼 라벨 셀렉터를 통해 포드를 생성한다. 따라서 라벨 셀렉터가 일치하는 포드를 같은 YAML에서 정의함
    [3]: 데몬셋도 다른 오브젝트처럼 포드를 기본 단위로 사용하기 때문에, 마스터 노드에 설정되어 있는 Taint를 인식한 상태로 포드가 할당된다.
    따라서 마스터 노드에도 포드를 생성하기 위해 간단한 Toleration을 하나 설정했다. (필수는 아니다.)
    [4]: 데몬셋은 일반적으로 노드에 대한 에이전트 역할을 하기 때문에 자원 부족 등으로 인해 데몬셋의 포드가 중지되는 것은 바람직하지 않다. 따라서 데몬셋을 
    생성할 때는 Guaranteed 클래스로 설정하는 것이 좋다. (필수는 아니다.)
    
    위의 YAML로 데몬셋을 생성하면 쿠버네티스의 모든 노드에 포드가 생성될 것이다.
        'kubectl apply -f daemonset-example.yaml'
        'kubectl get pods'
        NAME                    READY  STATUS  RESTARTS AGE 
        daemonset-example-2w9zl  1/1  Running     0     116s
        daemonset-example-96fc4  1/1  Running     0     116s
        daemonset-example-btv9p  1/1  Running     0     116s 
        daemonset-example-j6cpd  1/1  Running     0     116s
        
        'kubectl get daemonsets'
        NAME DESIRED     CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE 
      daemonset-example     4      4       4         4       4    <none>  2m
    
    만약 특정 노드에만 데몬셋의 포드를 생성하고 싶다면 노드 셀렉터(nodeSelector), Node Affinity를 포드에 적용할 수도 있다. 
    
        {
            데몬셋의 목적은 노드마다 포드를 하나씩 생성하는 것이기 때문에 노드에 장애가 발생했을 때에도 포드가 다른 노드로 퇴거(Evit)되지 않아야 한다.
            이를 위해 데몬셋의 포드에는 Toleration이 기본적으로 설정돼 있다.  
        }
        
        
        
                > 3. 스테이트풀셋(StatefulSets)
            > 스테이트풀셋 사용하기
    쿠버네티스에서 마이크로서비스 구조로 동작하는 애플리케이션은 대부분 상태를 갖지 않는 (Stateless) 경우가 많다. 그러한 경우에는 디플로이먼트를 통해 쉽게
    애플리케이션을 배포할 수 있지만, 데이터베이스처럼 상태를 갖는(Stateful) 애플리케이션을 쿠버네티스에서 실행하는 것은 매우 복잡한 일이다. 포드 내부의
    데이터를 어떻게 관리해야 할지, 상태를 갖는 포드에는 어떻게 접근할 수 있는지 고려해야하기 떄문이다. 
    
    쿠버네티스가 이에 대한 해결책을 완벽하게 제공하지는 않지만, 스테이트풀셋(StatefulSets)이라는 쿠버네티스 오브젝트를 통해 어느 정도는 해결할 수 있도록
    제공하고 있다. 이름에서 유추할 수 있듯이 스테이트풀셋은 상태를 갖는(Stateful) 포드를 관리하기 위한 오브젝트이다. 스테이트풀셋을 사용해보기에 앞서,
    쿠버네티스에서는 상태를 갖는 포드와 그렇지 않은 포드를 어떻게 여기는지 간단히 짚고 넘어가보자.
    
    쿠버네티스에서 상태가 없는 포드를 지칭할 때는 흔히 '가축'에 비유한다. 목장에서 풀어놓고 키우는 가축에는 이름을 지어주지도 않고, 개별 동물을 구분하지도 않는다.
    우리 입장에는 가축은 언제든 대체될 수 있는 동일한 개체로 보기 때문이다. 때문에 쿠버네티스에서는 상태가 없는 애플리케이션, 즉 디플로이먼트 등을 통해 배포되는
    포드를 보통 가축에 비유한다. 각 포드를 특별하게 여기지 않으며, 언제든지 생성되고 사라질 수 있기 떄문이다. 
    
    이와 반대로 쿠버네티스에서 상태가 존재하는 포드를 지칭할 때는 '애완동물'에 비유한다. 애완동물은 이름을 붙이기 때문에 구분이 된다. 따라서 사람의 입장에서
    애완동물은 대체 불가능한 개체로서, 항상 고유한 식별자를 갖는 것으로 여겨진다. 이와 같은 이유로 쿠버네티스에서는 상태를 갖는 애플리케이션, 즉 스테이트풀셋
    을 통해 생성되는 포드를 보통 애완동물에 비유한다. 상태를 갖는 각 포드를 모두 고유하며, 쉽게 대체될 수 없기 때문이다. 
        {
            쿠버네티스에서는 예전에 스테이트풀셋을 펫셋(PetSet)이라고 불렀다.
        }
    간단한 스테이트풀셋을 생성해보기 위해 아래의 내용으로 YAML 파일을 작성한다. 
```
statefulset-example.yaml
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: statefulset-example
spec:
  serviceName: statefulset-service
  selector:
    matchLabels:
      name: statefulset-example
  replicas: 3
  template:
    metadata:
      labels: 
        name: statefulset-example
    spec:
      containers:
      - name: statefulset-example
        image: alicek106/rr-test:echo-hostname
        ports:
        - containerPort: 80
          name: web
---
apiVersion: v1
kind: Service
metadata:
  name: statefulset-service
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    name: statefulset-example
```
```dockerfile
    
        'kubectl apply -f statefulset-example.yaml'
        -> 1개의 스테이트풀셋(3개의 포드) 그리고 1개의 서비스가 생성될 것이다.
        
        'kubectl get statefulset' #'kubectl get sts'로도 가능하다.
        
        'kubectl get pods'
                NAME        ~
        statefulset-example-0
        statefulset-example-1
        statefulset-example-2
        
    디플로이먼트에서 생성된 포드는 랜덤한 이름이 붙지만 스테이트풀셋으로 생성된 포드는 0,1,2...와 같이 숫자가 뒤에 붙는다. 스테이트 풀셋에는 이와같이 포드 이름
    에 붙여지는 숫자로 각 포드를 고유하게 식별한다.
    
        {
            스테이트풀셋에서 replicas의 값을 여러 개로 설정해 생성할 경우 기본적으로 0번 포드부터 차례로 생성되며, 이전 번호의 포드가 완전히 준비돼야
            다음 번호의 포드가 생성된다. 이 설정은 YAML의 spec.podManagementPolicy 에서 변경할 수 있다.
        }
    이 YAML 스테이트풀셋은 라벨 셀렉터, replicas, 포드 템플릿 등을 정의하고 있으며, 이것만 봤을 때는 데몬셋이나 디플로이먼트와 크게 달라 보이지 않는다.
    하지만 spec.serviceName이라는 항목을 정의하고 있으며, 이 항목에는 스테이트풀셋의 포드에 접근할 수 있는 서비스 이름을 입력해야한다. 
    왜 스테이트풀셋만 이런 항목이 있는걸까?
    
    스테이트풀셋에서 생성되는 포드는 모두 고유하다. 각자가 다른 개체로 취급된다. 만약 이러한 상황에서 디플로이먼트에서 사용했던 일반적인 서비스를 스테이트풀셋
    으로 사용하면 어떻게 될까? 서비스는 기본적으로 라벨셀렉터가 일치하는 랜덤한 포드를 선택해서 트래픽을 전달하기에 스테이트풀셋의 랜덤한 포드들에게 요청이
    분산될 것이다. 하지만 이것은 스테이트 풀셋이 바라는 바가 아니다. 스테이트풀셋의 각 포드는 고유하게 식별돼야 하며, 포드에 접근할 때에도 '랜덤한 포드'가
    아니라 '개별 포드'에 접근해야한다. 
    
    이러한 경우, 일반적인 서비스가 아닌 헤드리스 서비스(Headless Service)를 사용할 수 있다. 헤드리스 서비스는 서비스 이름으로 포드 접근 위치를 알아내기
    위해서 사용되며, 서비스의 이름과 포드의 이름을 통해서 포드에 접근할 수 있다. 위 YAML에서 ClusterIP가 None으로 되어 있는데, 이것이 헤드리스 서비스
    라는 것을 의미한다. ClusterIP가 None이기에 서비스 목록에서도 ClusterIP가 출력되지 않는다. 헤드리스 서비스는 SRV 레코드로 쓰이기 때문에 헤드리스
    서비스의 이름을 통해 포드에 접근할 수 있는 IP를 반환할 수 있다. 예를 들어 nslookup 명령어에 헤드리스 서비스 이름을 입력하면 접근 가능한 포드의 IP를
    출력한다.
    
        'kubectl run -i --tty --image busybox:1.28 debug --restart=Never --rm nslookup statefulset-service'
                 
             Server: 10.96.0.10
             Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
             
             Name: statefulset-service 
             Address 1: 192.168.2.98 statefulset-example-0.statefulset-service.default.svc.cluster.local
             Address 2: 192.168.3.16 statefulset-example-1.statefulset-service.default.svc.cluster.local
             Address 3: 192.168.1.153 statefulset-example-2.statefulset-service.default.svc.cluster.local
    
    nslooup을 출력 결과에서 알 수 있듯 <포드 이름>.<서비스 이름>을 통해서도 포드에 접근할 수 있다. 스테이트풀셋에서 포드의 이름은 0부터 시작하는 숫자가
    붙기 때문에 쿠버네티스 클러스터 내부에서는 고유한 포드 이름으로 접근할 수 있다.  디플로이먼트에서 생성했던 포드에 접근할 떄는 이처럼 헤드리스 서비스를 사용
    하지 않았던 것을 생각하면 서두에 가축과 애완동물 비유를 이해할 수 있을 것이다. 이러한 개념은 쿠버네티스뿐만 아니라 데브옵스(DevOps)에서도 자주 등장하기
    때문에 스테이트 풀셋이 의도하는 바를 정확히 알고 넘어가는 것이 좋다. 
    
            > 스테이트풀셋과 퍼시스턴트 볼륨
    앞서 사용했던 스테이트풀셋 예지는 단순히 포드 컨테이너의 호스트 이름을 반환하는 단순한 웹 서버를 생성했다. 하지만 실제로 스테이트 풀셋을 사용할 때는
    포드 내부에 저장되는, 상태가 존재하는(Stateful) 애플리케이션을 배포하는 경우가 대부분일 것이다. 이전에 살펴봤듯 포드의 데이터는 일반적으로 휘발성이기 
    때문에 포드 내부에 퍼시스턴트 볼륨을 마운트해 데이터를 보존하는 것이 일반적이다. 
    
    스테이트풀셋도 마찬가지로 퍼시스턴트 볼륨을 포드에 마운트해 데이터 보관하는 것이 바림직하다. 하지만 스테이트풀셋의 포드가 여러 개라면 포드마다 퍼시스턴트 볼륨
    클레임을 생성해줘야하는 번거로움이 있다. 다행히도 쿠버네티스는 스테이트풀셋을 생성할 떄 포드마다 퍼시스턴트 볼륨 클레임을 자동으로 생성함으로써
    다이나믹 프로비저닝 기능을 사용할 수 있도록 지원한다.          
             
    이 기능은 아래의 YAML 파일처럼 스테이트풀셋에서 spec.volumeClaimTemplates 항목을 정의함으로써 사용할 수 있다.
    
        {
            아래의 YAML은 AWS의 kops 환경에서 generic이라는 이름의 스토리지 클래스를 미리 설정한 후를 가정한 YAML이다.
        }
```
```yaml
...
        name: web
      volumeMounts:
      - name: webserver-files
        mountPath: /var/www/html/
  volumeClaimTemplates:
  - metadata:
      name: webserver-files
    spec:
      accessModes: ['ReaWriteOnce']
      storageClassName: generic
      resources:
        requests:
          storage: 1Gi
```
```dockerfile
    volumeClaimTemplates를 사용하면 스테이트풀셋의 각 포드에 대해 퍼시스턴트 볼륨 클레임이 생성된다. volumeClaimTemplates라는 단어가 의미하는 것
    처럼 포드가 사용할 퍼시스턴트 볼륨 클레임의 템플릿을 정의하는 것이라고 생각하면 이해하기 쉽다. volumeCalimTemplates에 정의된 설정대로 퍼시스턴트 볼륨
    이 동적으로 생성될 것이고, 이는 각 포드에 마운트 된다.
    
            'kubectl apply -f statefulset-volume.yaml'
            'kubectl get pv,pvc'
            
    단, 스테이트풀셋을 삭제한다고 해서 volumeClaimTemplate로 생성된 퍼시스턴트 볼륨과 클레임은 직접 삭제해야 한다.
    
```

