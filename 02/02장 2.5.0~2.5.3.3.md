### 2.5 도커 데몬 
#### 2.5.1 도커의 구조
```dockerfile
    도커를 사용할 때 docker라는 명령어를 사용했다 도커의 위치는 실제로 어디에 있는 것일까
       
       ` which docker
        /usr/bin/docker`
        
    프로세스로 확인해보면
    
        'ps -ef | grep docker'
```
| | | | | 
|---|---|---|---|
|root|         867|    00:00:38 |/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock|
|root|        1326|    00:00:00 |/usr/bin/docker-proxy -proto tcp -host-ip 192.168.0.23 -host-port 1247 -container-ip 172.17.0.2 -container-port 1247|
|root|        1338|    00:00:00 |/usr/bin/docker-proxy -proto tcp -host-ip 192.168.0.23 -host-port 3206 -container-ip 172.17.0.3 -container-port 3306|
|root|        4276|    00:00:00 |/bin/sh -c sudo docker exec blog /usr/bin/java -jar /home/projects/BLOG.war|
|root|        4277|    00:00:00 |sudo docker exec blog /usr/bin/java -jar /home/projects/BLOG.war|
|root|        4278|    00:00:16 |docker exec blog /usr/bin/java -jar /home/projects/BLOG.war|
|root|        5918|    00:00:00 |grep --color=auto docker|

```dockerfile
    컨테이너나 이미지를 다루는 명령어는 /usr/bin/docker에서 실행되지만 도커 엔진의 프로세스는 /usr/bin/dockerd 파일로 실행
    되고 있다. 이는 docker 명령어가 실제 도커 엔진이 아닌 클라이언트로서의 도커이기 때문이다.
    
    
    도커의 구조는 크게 두 가지로 나뉜다. 하나는 클라이언트로서의 도커이고, 다른 하나는 서버로서의 도커이다. 실제로 컨테이너를 생성하고
    이미지를 관리하는 주체는 도커 서버이고 이는 dockerd 프로세스로서 동작한다. 도커 엔진은 외부에서 API입력을 받아 도커 엔진의 기능을
    수행하는데, 도커 프로세스가 실행되어 서버로서 입력을 받을 준비가 된 상태를 도커 데몬이라고 이야기한다.
    
    다른 하나는 도커 클라이언트이다. 도커 데몬은 API 입력을 받아 도커 엔진의 기능을 수행하는데, 이 API를 사용할 수 있도록 CLI를 
    제공하는 것이 도커 클라이언트이다. 사용자가 docker로 시작하는 명령어를 입력하면 도커 클라이언트를 사용하는 것이며, 도커 클라이언트
    는 입력된 명령어를 로컬에 존재하는 도커 데몬에게 API로서 전달한다. 이때 도커 클라이언트는 /var/run/docker.sock에 위치한
    유닉스 소켓을 통해 도커 데몬의 API를 호출한다. 도커 클라이언트가 사용하는 유닉스 소켓은 같은 호스트 내에 있는 도커 데몬에게 명령
    을 전달할 때 사용된다. tcp로 원격에 있는 도커 데몬을 제어하는 방법도 있다.
```
-----
`
        USER ⎯ docker version ⎯→ DOCKER CLIENT ⎯ /var/run/docker.sock ⎯→ DOCKER DAEMON
----


#### 2.5.2 도커 데몬 실행
```dockerfile
    도커 데몬은 일반적으로 아래와 같은 명령어 시작, 정지할 수 있습니다. 우분투에서 도커가 설치되면 자동으로 서비스로 등록이 
    되므로 호스트가 재시작되더라도 자동으로 실행된다.
    
        'service docker start'
        'service docker stop'
        
        {
            레드햇 계열은 도커를 설치해도 자동으로 실행되도록 설정되지 않는다. 도커를 자동으로 실행하도록 설정하려면
            'systemctl enable docker'로 서비스를 활성화 한다.
        }
        
    앞에서 설명했듯이 도커 서비스는 dockerd로 도커 데몬을 실행한다. 그러나 서비스를 사용하지 않고 직접 도커 데몬을 실행할 수도 있다.
    dockerd 명령어 또한 /usr/bin/dockerd로서 존재하기 때문에 docker 명령어와 같이 바로 사용할 수 있다.
    
        'dockerd' > 명령어를 사용하면 도커 데몬이 실행된다. 
    
    그러면 각종 정보가 출력되는데, 마지막에 /var/run/docker.sock에서 listen 할 수 있는 상태라는 메시지가 출력된다.
    터미널을 하나 더 연 다음 도커 명령어를 입력하면 이전처럼 도커를 사용할 수 있다. 
   
    {
        디버깅이나 도커 자체의 트러블슈팅이 필요하다면 도커 데몬을 직접 실행하는 것이 더 나을 수 있다. 
    }
```

#### 2.5.3 도커 데몬 설정 
```dockerfile
    도커 데몬에 적용할 수 있는 옵션을 살펴보자 'dockerd --help' 명령어로 확인할 수 있다. docker와 마찬가지로 옵션을 붙여서
    dockerd를 실행시킬 수도 있다. 
    
        'dockerd --insecure-registry=192.168.99.100:5000'
    
    그러나 dockerd 명령어로 도커 데몬을 직접 실행하는 것보다 도커 설정파일을 수정한 뒤 도커 데몬이 설정파일을 읽어 서비스로 
    실행하게 하는 것이 일반적이다. 실제로 사용할 때는 옵션을 그대로 설정 파일의 DOCKER_OPTS에 입력하면 된다.
```

#### 2.5.3.1 도커 데몬 제어 : -H
```dockerfile
    -H 옵션은 도커 데몬의 API를 사용할 수 있는 방법을 추가한다. 아무런 옵션을 설정하지 않고 도커 데몬을 실행하면 
    도커 클라이언트를 위한 유닉스 소켓인 /var/run/docker.sock을 사용한다. 그러므로 단순히 dockerd를 입력해 
    도커 데몬을 실행해도 도커 클라이언트의 CLI를 사용할 수 있다. 
    
        'dockerd'
        'dockerd -H unix:///var/run/docker.sock'
    
    -H에 IP주소와 포트 번호를 입력하면 원격 API인 Docker Remote API로 도커를 제어할 수 있다. Remote API는
    도커 클라이언트와 다르게 로컬에 있는 도커 데몬이 아니더라도 제어할 수 있으며, RESTful API 형식을 띠고 있으므로
    HTTP 요청으로 도커를 제어할 수 있다.
    
    -H에 유닉스 소켓을 지정하지 않으면 유닉스 소켓은 비활성화되므로 도커 클라이언트를 사용할 수 없게 된다. 즉 docker로 시작되는 
    명령어를 사용할 수 없다. 따라서 일반적으로 도커 클라이언트를 위한 유닉스 소켓과 Remote API를 함께 설정한다.
    
        'dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0.2375'
    
    -H로 Remote API를 사용하려면 cURL 같은 HTTP 요청 도구를 사용한다. 
    
        'dockerd -H tcp://192.168.99.100:2375'
        
        > curl 192.168.99.100:2375/version --silent | python -m json.tool
        
    쉘의 환경변수를 설정해서 원격에 있는 도커를 제어할 수도 있다. 도커 클라이언트는 셸의 DOCKER_HOST 변수가 설정돼 있다면
    해당 도커 데몬에 API 요청을 전달한다. 
    
        'export DOCKER_HOST="tcp://192.168.99.100:2375"'
        'docker version'
        
    반대로 도커 클라이언트에  -H로 제어할 원격 도커 데몬을 설정할 수도 있다.
        
        'docker -H tcp://192.168.99.100:2375 version'
```

#### 2.5.3.2 도커 데몬에 보안 적용 : --tlsverify
```dockerfile
    도커를 설치하면 기본적으로 보안 연결이 설정돼 있지 않다. 이는 Remote API를 사용할 떄 별도 보안이 없음을 의미한다.
    그러나 실제 운영 환경에서 도커를 사용할 때는 보안을 적용하는 것이 바람직하다.  보안을 적용할 때 사용될 파일은
    ca.pem, server-cert.pem, server-key.pem, cert.pem, key.pem이고 클라이언트가 도커 데몬에 접근하려면 
    ca.pem, cert.pem, key.pem 파일이 필요하다. 
    
    1. 서버측 파일 생성
       1) 인증서에 사용될 키를 생성한다.
        'openssl genrsa -aes256 -out ca-key.pem 4096'
        
       2) 공용 키를 생성한다.
        'openssl req -new -x509 -days 10000 -key ca-key.pem -sha256 -out ca.pem'
        
       3) 서버측 키를 생성한다.
        'openssl genrsa -out server-key.pem 4096'
        
       4) 서버측에서 사용될 인증서를 위한 인증 요청서 파일을 생성한다. $HOST 부분에는 사용 중인 도커 호스트의 IP주소
       또는 도메인이며, 외부에서 접근 가능한 IP 또는 도메인이어야한다. 
        'openssl req -subj "/cn=$HOST" -sha256 -new -key server-key.pem -out server.csr'
        
       5) 접속에 사용될 IP주소를 extfile.cnf 파일로 저장한다. $HOST는 도커 호스트의 IP 또는 도메인이다.
        'echo subjectAltName = IP:$HOST, IP:127.0.0.1 > extfile.cnf'
        
       6) 서버측의 인증서 파일을 생성한다. 
        'openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem
        -CAcreateserial -out server-cert.pem -extfile extfile.cnf'
        
    2. 클라이언트 측에서 파일 생성
        1) 클라이언트 측의 키 파일과 인증 요청 파일을 생성하고, extfile.cnf 파일에 extendedKeyUsage 항목을 추가한다.
         'openssl genrsa -out key.pem 4096'
         'openssl req -subj `/CN=client` -new -key key.pem -out client.csr'
         'echo extendedKeyUsage = clientAuth > extfile.cnf'
        
        2) 클라이언트 측의 인증서를 생성한다.
         'openssl x509 -req -days 30000 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem
          -CAcreateserial -out cert.pem -extfile extfile.cnf' 
          
        3) 생성된 파일의 쓰기 권한을 없애 읽기 전용으로 만든다.
         'chmod -v 0400 ca-key.pem key.pem server-key.pem ca.pem server-cert.pem cert.pem'
         
        4) 도커 데몬의 설정 파일이 존재하는 디렉토리인 ~/.docker로 도커 데몬 측에서 필요한 파일을 옮긴다. 필수는 아니다.
         cp {ca, server-cert, server-key, cert, key}.pem ~/.docker 
    
    
    보안 적용을 위한 파일을 모두 생성했으므로 TLS 보안적용을 활성화하기 위해 -tlsverify 옵션을 추가하고, --tlscacert,
    --tlscert, --tlskey에는 각각 보안을 적용하는데 필요한 파일의 위치를 입력한다.
    
        'dockerd --tlsverify \ 
        --tlscacert=/root/.docker/ca.pem \
        --tlscert=/root.docker/server-cert.pem \
        --tlskey=/root/.docker/server-key.pem \
        -H=0.0.0.0:2376 \
        -H unix:///var/run/docker.sock'
    
    클라이언트에서 -H를 추가해 보안이 적용된 도커를 제어해보자 'docker -H 192.168.99.100:2376 version'
         
    {
        도커의 Remote API를 사용하는 포트는 보안이 적용돼 있지 않다면 2375번 포트를, 보안이 적용돼 있다면 2376번 포트를 
        사용하도록 설정한다.         
    }
    
    TLS 연결 설정을 하지 않았다는 에러가 출력되는 것을 볼 수 있다. 보안이 적용된 도커 데몬을 사용하려면 ca.pem, key.pem,
    cert.pem 파일이 필요하다. 이 파일을 docker 명령어의 옵션에 명시한 명령어는 아래와 같다.
    
        'docker -H 192.168.99.100:2376 \
        --tlscacert=/root/.docker/ca.pem \
        --tlscert=/root/.docker/cert.pem \
        --tlscert=/root/.docker/key.pem \
        --tlsverify version
    
    위의 명령어를 매번 입력하는 것은 성가시다. 이또한 셸의 DOCKER_HOST 환경변수와 마찬가지로 인증 관련 환경변수를 
    설정해 매번 파일의 위치를 입력하지 않게 설정할 수 있다. DOCKER_CERT_PATH는 도커 데몬 인즈엥 필요한 파일 위치를, 
    DOCKER_TLS_VERIFY는 TLS 인증을 사용할지를 설정한다. 
    
        'export DOCKER_CERT_PATH="/root/.docker"'
        "export DOCKER_TLS_VERIFY=1"
        
    {
        셸의 환경변수는 셸이 종료되면 초기화되므로 ~./bashrc등의 파일에 export를 추가해 셸을 사용할때마다 
        환경변수의 값을 설정할 수 있다. 
        
        vi ~/.bashrc
        
        export DOCKER_CERT_PATH="root/.docker"
        export DOCKER_TLS_VERIFY=1
    }
    
    curl로 보안이 적용된 도커 데몬의 Remote API를 사용하려면 플래그를 추가하면 된다.
        'curl https://192.168.99.100:2376/version \
        --tlscacert=/root/.docker/ca.pem \
        --tlscert=/root/.docker/cert.pem \
        --tlscert=/root/.docker/key.pem'
```

#### 2.5.3.3 도커 스토리지 드라이버 변경 --storage-driver
```dockerfile
1.도커 스토리지 드라이버 변경 --storage-driver

   도커는 특정 스토리지 백엔드 기술을 사용해서 도커 컨테이너와 이미지를 저장하고 관리한다. 일부 운영 체제는 도커를 설치할 때 
   기본적으로 사용하도록 설정된 스토리지 드라이버가 있는데, 우분투 같은 데비안 계열 운영체제는 overlay2를, 구 버전은 CentOS는
   deviceampper를 사용하는 것이 예시이다. 이는 'docker info'로 확인할 수 있다. 
   
        'docker info | grep "Storage Driver"'
        Storage Driver : overlay2
        
   도커를 사용하는 환경에 따라 스토리지 드라이버는 자동으로 정해지지만 도커 데몬 실행 옵셔넹서 스토리지 드라이버를 변경할 수도 있다.
   스토리지 드라이버는 도커 데몬 옵션 중 --storage-driver를 사용해서 선택할 수 있으며, 지원하는 드라이버로는 OverlayFS,
   AUFS, Btrfs, Devicemapper, VFS, ZFS 등이 있다. 이 가운데 하나만 선택해 도커 데몬에 적용할 수 있으며, 적용된 스토리지
   드라이버에 따라 컨테이너와 이미지가 별도로 생성된다. 
   
   예를 들어, 도커가 AUFS를 기본적으로 사용하도록 설정된 우분투에서 다음과 같이 도커 데몬을 실행하면 별도의 Devicemapper 컨테이너와 이미지를 사용하므로
   AUFS에서 사용했던 이미지와 컨테이너를 사용할 수 없다. 별도로 생성된 Devicemapper 파일은 /var/lib/docker/devicemaper 디렉토리에 저장되며, 
   AUFS드라이버 또한 /var/lib/docker/aufs 디렉토리에 저장된다.
   
        'dockerd --storage-driver=devicemapper'
   
   어떤 스토리지 드라이버를 사용할지는 개발하는 컨테이너 애플리케이션 및 개발 환경에 따라 다르다. 




2. 스토리지 드라이버의 원리
    이미지는 읽기 전용 파일로 사용되며 컨테이너는 이 이미지 위에 얇은 컨테이너 레이어를 생성함으로써 컨테이너의 고유한 공간을 생성한다는 것이었다.
    실제로 컨테이너 내부에서 읽기와 새로운 파일 쓰기, 기존의 파일 쓰기 작업이 일어날 때는 드라이버에 따라 Copy-on-Write 또는 Redirect-on-Write
    의 개념을 사용한다. 
    
        > 스냅샷의 기본 개념은 '원본 파일은 읽기 전용으로 사용하되 이 파일이 변경되면 새로운 공간을 할당한다'는 것이다. 스토리지 스냅샷을 만들면 스냅샷
        안에 어느 파일이 어디에 저장돼 있는지가 목록으로 관리된다. 그리고 스냅샷을 사용하다. 스냅샷 안의 파일에 변화가 생기면 변경된 내역을 따로 관리함
        으로써 스냅샷을 사용한다. 예를 들어 파일 A, B, C를 읽을 때는 단순히 읽으면 된다. 쓰기 작업 때는 상황이 조금 다른데, 원본 파일을 유지하면서
        변경된 사항을 저장할 수 있어야 하기 때문이다. 이를 해결하는 방법에 따라 CoW, RoW로 나뉜다. 
        
        CoW는 스냅샷의 파일에 쓰기 작업을 수행할 때 스냅샷 공간에 원본 파일을 복사한 뒤 쓰기 요청을 반영한다. 이 과정에서 복사하기 위해 파일을  읽는
        작업 한 번, 파일을 스냅샷 공간에 쓰고 변경된 사항을 쓰는 작업이 총 2번의 쓰기 작업이 일어나므로 오버헤드가 발생한다.
           
            * 오버헤드(overhead)는 어떤 처리를 하기 위해 들어가는 간접적인 처리 시간 · 메모리 등을 말한다.

        RoW는 CoW와 다르게 한 번의 쓰기 작업만 일어난다. 이는 파일을 스냅샷 공간에 복사하는 것이 아니라 스냅샷에 기록된 원본 파일은 스냅샷 파일로 묶은
        뒤 변경된 사항을 새로운 장소에 할당 받아 덮어쓰는 형식이다. 
        
    
    도커에서 이미지 레이어는 각 스냅샷에 해당하고, 컨테이너는 이 스냅샷을 사용하는 변경점으로 이해하면 된다. 컨테이너 레이어에는 이전 이미지에서
    변경된 사항이 저장돼 있으며, 컨테이너를 이미지로 만들면 변경된 사항이 스냅샷으로 생성되고 하나의 이미지 레이어로 존재하게 된다. 
    
     
     - 1. AUFS 드라이버 사용하기
     AUFS 드라이버는 데비안 계열에서 기본적으로 사용할 수 있는 드라이버 이며, 도커에서 오랜 기간 사용했기 떄문에 안정성 측면에서 우수하다고 평가받는다.
     그러나 AUFS 모듈은 기본적으로 커널에 포함돼 있지 않으므로 일부 원영체제에서는 사용할 수 없으며, 사용할 수 없는 대표적인 운영 체제는 RHEL,
     CentOS 등이다.
     
     AUFS 드라이버는 지금까지 알아본 이미지의 구조와 유사하다. 여러 개의 이미지 레이어를 Union Mount Point로 제공하며, 컨테이너 레이어는 여기에
     마운트해서 이미지를 읽기 전용으로 사용한다. 
     
            'DOCKER_OPTS="--storage-driver=aufs"'
     
            * (unionMountPoint: /var/lib/docker/aufs/mnt , containerLayer: /var/lib/docker/aufs/diff)
     AUFS 드라이버는 컨테이너에서 읽기 전용으로 사용하는 파일을 변경해야 한다면 컨테이너 레이어로 전체 파일을 복사하고, 이 파일을 변경함으로써 변경 사항을
     반영한다. 복사할 파일을 찾기 위해 이미지의 가장 위부터 아래로 찾기 때문에 큰 파일이 이미지의 아래쪽 레이어에 있다면 시간이 오래 걸릴 수 있다. 그러나
     한 번 파일이 컨테이너 레이어로 복사되면 그 뒤로는 이 파일로 쓰기를 작업을 수행한다.
     
     AUFS는 컨테이너의 실행 삭제 등의 컨테이너 관련 수행 작업이 매우 빠르므로 PaaS에 적합한 드라이버로 꼽힌다.
            * PaaS : Platform as a Service
            
            
     - 2. Devicemapper 드라이버 사용하기
     Devicemapper 드라이버는 레드햇 계열의 리눅스 배포판을 위해 개발된 스토리지 드라이버이다. 대부분의 리눅스 배포판에서 보편적으로 사용할 수 있지만
     성능상의 이유로 deprecated된 드라이버이다. 따라서 호스트 커널 버전이 너무 낮거나 하지 않으면 overlay, overlay2를 사용하는 것이 좋다.
     
            'DOCKER_OPTS="--storage-driver=devicemapper"'
            
     Devicemapper 드라이버를 사용하도록 설정한 뒤, /var/lib/docker/devicemapper/devicemapper 디렉토리를 살펴보면 data, metadata를 
     볼 수 있다. 여기서 data는 이미지와 컨테이너의 데이터가 분리된 디렉토리로 저장되는 것이 아닌 data 파일로 구성하는 pool에서 블록 단위로 할당 받는
     구조이다. 컨테이너와 이미지 블록 정보는 metadata 파일에 저장된다. 
       
       {
            ls 명령어에 -lash를 추가하면 실제 도커 데몬이 차지하는 파일의 크기를 확인할 수 있다.
            ls -lsah /varlib/docker/devicemapper/devicemapper
       }
       
     Devicemapper 드라이버를 사용하는 도커는 devicemapper 스토리지 풀에서 공간을 할당받고, 이미지 스냅샷을 만들어 상위 레이어를 생성한다.
     이미지로부터 변경된 사항을 저장하는 컨테이너 레이어는 이 레이어들을 묶은 마운트 포인트에서 새로운 스냅샷을 생성해서 사용한다.
     
     devicemapper 드라이버를 사용하는 컨테이너는 allocate-on-demand라는 원리로 컨테이너 내부에서 새로운 파일을 기록한다. 이는 devicemapper
     의 풀에서 필요한 만큼 64kb 크기의 블록 개수를 할당해서 쓰기 작업을 수행하는 것이다. 컨테이너 내부에 이미 존재하는, 즉 이미지에 존재하던 데이터에 
     쓰기 작업을 수행할 떄는 변경하려는 원본 파일의 블록을 컨테이너에 복사한 뒤, 컨테이너 내부에 복사된 블록 파일을 수정한다. 
     
     이는 AUFS 드라이버와 달리 전체 파일을 복사하지 않는다는 점이 성능상 이점이지만 devicemapper는 컨테이너를 생성하고 삭제하는 등의 작업이 빠른 편이
     아니다.
     
     Devicemapper 드라이버는 devicemapper라는 데이터 풀 안에 컨테이너와 이미지 데이터를 저장하기 때문에 Multitenancy 환경을 위해 각 컨테이너와
     이미지를 분리해 관리하는 것이 불가능하다.
            * 멀티 테넌시(Multi-tenancy)는 소프트웨어 애플리케이션의 단일 인스턴스가 여러 고객에게 서비스를 제공하는 아키텍처다.
     따라서 PaaS와 같은 용도로 사용하는 것을 권장하지 않는다. 굳이 사용하려면 스토리지 풀로 loop-lvm이 아닌 direct-lvm을 사용하는 것이 좋다.
     
     
     - 3. OverlayFS 드라이버 사용하기 
     OverlayFS는 레드햇 계열 및 라즈비안, 우분투 등 대부분 운영체제에서 도커를 설치하면 자동으로 사용되도록 설정되는 드라이버이다. OverlayFS는
     AUFS와 비슷한 원리로 동작하지만 좀 더 간단한 구조로 사용되며 성능 또한 조금 더 좋기 떄문에 OverlayFS를 기본적으로 사용한다. 
     
     OverlayFS는 overlay와 overlay2 드라이버로 나뉜다. overlay는 커널 3.18이상부터 내장되어 있다. overlay2는 4.0버전 이상에서 사용할 수 
     있다. overlay2가 overlay보다 성능이 약간 우수하며, 이미지를 구성하기 위해 여러 개의 레이어 구조를 지원한다. overlay는 다른 스토리지 드라이버
     와는 달리 계층화된 이미지 구조를 사용하지 않으며, lowerdir이라는 단일화된 이미지 레이어를 사용한다. 
      
            'DOCKER_OPTS="--storage-driver overlay"'
    
     이 후 'docker run -it --name container ubuntu:14.04'로 컨테이너를 생성하고 'echo my file! >> overfile'로 변경 사항을 만든다.
     호스트를 빠져나와 /var/lib/docker/overlay 디렉토리 내용을 살펴보면 컨테이너와 이미지 파일을 담고 있는 디렉토리가 존재하는 것을 확인할 수 있다.
     이 가운데 -init이 붙은 디렉토리가 방금 생성한 컨테이너를 의미한다. 이 디렉토리의 이름에서 -init을 제외한 디렉토리가 실제 컨테이너의 파일 시스템을
     담고 있는 디렉토리이다. -init을 제외한 이름의 디렉토리를 확인하면 4개의 파일이 존재한다.
     
        [lower-id, merged, upper, work]

     {
        mount 명령어를 사용하면 컨테이너 마운트 디렉토리를 쉽게 확인할 수 있다.
        
        'mount | grep overlay'
     }
     
     overlay 드라이버는 컨테이너를 사용하기 위해 도커를 merged, upperdir, lowerdir의 구조로 나눈다. lowerdir은 도커의 이미지 레이어에 해당하고
     upperdir은 컨테이너 레이어에 해당한다. 다른 스토리지 드라이버와는 다르게 여러 계층의 이미지 레이어가 존재하는 것이 아니며, 여러 개의 이미지 레이어
     를 하나의 컨테이너 마운트 지점에 통합하여 사용한다.
     
     upperdir에는 컨테이너에서 발생한 변경사항을 담고 있으며, 위에서 생성한 컨테이너는 overlayfile이라는 변경 사항을 가지고 있기 때문에 upperdir
     디렉토리에는 overlayfile 파일이 존재한다. 그리고 upperdir은 이미지 레이어에 해당하는 lowerdir과 함께 마운트되어 최종적으로 컨테이너 내부에
     보여지게 되고, 이것이 컨테이너 마운트 지점인 merged 디렉토리이다.
     AUFS와 유사하게 overlay는 이미지에 존재하는 파일에 쓰기 작업을 수행할 때 파일을 컨테이너 레이어인 upperdir로 복사(copy_up operation)해 
     사용한다. 따라서 크기가 큰 파일에 쓰기 작업을 수행할 떄는 upperdir에 복사하는 시간으로 인해 작업 수행에 지연이 생길 수 있으나, AUFS와는 다르게
     계층화된 다중 레이어 구조가 아니기 때문에 복사할 파일을 찾는 과정에서는 AUFS보다 빠르다. 
     
     
     - 4. Btrfs 드라이버 사용하기
     Btrfs는 리눅스 파일시스템 중 하나로, SSD 최적화, 데이터 압축 등 다양한 기능을 제공한다. Btrfs 드라이버는 Devicemapper나 AUFS, OverlayFS
     와는 다르게 파일시스템을 별도로 구성하지 않으면 도커에서 사용할 수 없으며, /var/lib/docker 디렉토리가 btrfs 파일 시스템을 사용하는 공간에 
     마운트돼 있어야만 도커는 Btrfs를 스토리지 드라이버로 인식한다. Btrfs는 리눅스 커널에 포함돼 있으므로 대부분의 리눅스 배포판에서 사용할 수 있다.
     
     
        1. Btrfs를 설치하기 전에 도커 데몬이 실행 중이라면 먼저 이를 정지한 뒤 진행한다.
            'service docker stop'
        
        2. Btrfs를 생성하기 위한 도구를 설치한다. 우분투와 같은 데비안 계열에서는 btrfs-tools를, 레드헷 계열에서는 btrfs-progs를 설치한다.
            [ debian ]
            'apt-get install btrfs-tools'
            
            [ redhat (CentOS) ]
            'yum install btrfs-progs'
            
        3. 명령어로 Btrfs 스토리지 풀을 생성한다.
            'mkfs.btrfs -f [디바이스 이름 ?? ex) /dev/xvdb]'
            
        도커를 아직 설치하지 않아 /var/lib/docker가 없다면 생성한다.
            'mkdir /var/lib/docker'    
        
        4. 시스템 재부팅 때마다 Btrfs 디바이스가 마운트 되도록 설정하기 위해 /etc/fstab에 아래 내용을 추가한다.
            'vi /etc/fstab
             
             [디바이스 이름] /var/lib/docker btrfs defaults 0 0
            '
            
            {
                /etc/fstab 파일은 리눅스 부팅시 파티션을 자동으로 마운트하도록 설정하는 파일이다.
            }
        
        5. /etc/fstab 파일에 설정한 내용을 적용하기 위해 'moutn -a' 명령어를 입력해 fstab 파일에 명시된 파일 시스템을 마운트한다.
            'mount -a'
    
        6. Btrfs 마운트가 완료됐으며, /var/lib/docker 디렉토리는 Btrfs 파일시스템을 사용하는 디바이스에 마운트 됐다. 도커를 시작한 뒤
        docker info 명령어로 스토리지 드라이버를 확인하면 Btrfs드라이버를 사용하고 있음을 알 수 있다.
        
     
     Btrfs 드라이버는 이미지와 컨테이너를 서브볼륨과 스냅샷 단위로 관리한다 구조 자체는 AUFS, devicemapper 등 다른 스토리지 드라이버와 유사
     하다. 이미지에서 가장 아래에 있는 베이스 레이어가 서브 볼륨이 되고, 그 위에 쌓이는 자식 레이어가 베이스레이어에 대한 스냅샷으로 생성된다.
     Btrfs는 devicemapper와 달리 블록 단위가 아닌 파일 단위로 각 레이어를 저장하기 때문에 /var/lib/docker/btrfs 디렉토리에서 이를 확인
     할 수 있다. 
     Btrfs 드라이버를 사용하는 컨테이너 내부에서 새로운 파일을 생성하는 것은 devicemapper와 마찬가지로 allocate-on-demand 작업에 의해 
     일어난다. 이미 존재하던 파일에 쓰기 작업을 수행할 때는 원본 파일을 보존하고 스냅샷에 새로운 공간을 할당하는 RoW 방식을 사용한다. 
     Btrfs는 자체적으로 SSD에 최적화돼 있으며 대체적으로 우수한 성능을 보여준다. 또한 리눅스의 파일시스템이 제공하지 않는 여러 기능을 제공한다는
     장점도 있다. 
     
     
     - 5. ZFS 드라이버 사용하기
     ZFS는 썬 마이크로시스템즈에서 개발했으며 Btrfs처럼 압축, 레플리카, 데이터 중복 제거 등 다양한 기능을 제공한다. 그러나 ZFS는 라이선스 문제로
     리눅스 커널에 기본적으로 탑재돼 있지 않게 떄문에 별도의 설치 과정이 필요하다. 이를 위해 ZFS on Linux라는 프로젝트의 모듈을 사용해야한다.
     
        1. 데몬 정지
            'service docker stop'
        
        2. zfs 설치
            'apt install zfsutils-linux'
            
        3. 모듈 로드
            'modprobe zfs'
            
        4. zpool을 생성, 이 zpool의 이름은 zpool-docker로 설정돼 생성된다.
            'zpool create -f zpool-docker [디바이스 이름]'
            
        5. ZFS 파일시스템을 생성하고 이를 /var/lib/docker에 마운트
            'zfs create -0 mountPoint=/var/lib/docker zpool-docker/docker'
    
        6. ZFS 풀이  /var/lib/docker에 정상적으로 마운트됐는지 확인한다. 
            'zfs list -t all'
            
        7. 도커를 재시작하고 'docker info'로 스토리지 드라이버가 zfs로 설정됐는지 확인한다.
            'service docker start'
            'docker info | grep Driver'
        {
            zpool이 마운트 됐지만 도커가 사용하지 않는다면
            DOCKER_OPTS="--storage-driver=zfs"
        }
        
     ZFS도 용어 차이만 있을뿐 다른 드라이버들과 유사하다. ZFS드라이버를 사용하면 이미지와 컨테이너 레이어는 ZFS 파일 시스템, ZFS클론, 
     ZFS 스냅샷으로 구분되어 관리된다. 이미지의 베이스 레이어가 ZFS 파일 시스템이되고, 이로부터 ZFS 스냅샷을 생성해 하나의 레이어를 구성한다. 이
     ZFS스냅샷에서 ZFS 클론을 생성해 이를 컨테이너 레이어를 위한 공간으로 사용한다. ZFS 클론을 다시 스냅샷으로 만들면 다시 이미지 레이어로서 생성되며,
     이전과 동일한 과정을 거쳐 하나의 이미지로 사용된다. 
     
     ZFS의 쓰기와 읽기 작업도 Btrfs와 유사하게 RoW를 사용한다. 컨테이너 레이어에서 새로운 데이터를 쓸 때는 allocate-on-demand 작업에 의해 zpool
     로부터 새로운 블록을 할당받고 쓰기 작업을 수행한다. 이미 존재하던 파일에 쓰기 작업을 수행할 때는 컨테이너 레이어인 ZFS 클론에  zpool로부터 128KB
     크기의 새로운 블록을 여러 개 할당하고 이 블록에 쓰기 작업을 시작한다.
     ZFS는 성능뿐만 아니라 안정성에 초점을 뒀으며, 압추고가 데이터 중복 제거 등 여러 기능을 제공한다. 또한 ZFS는 ARC(Adaptive Replacement Cache)
     라고하는 메모리 구조로 디스크 블록을 캐시하기 떄문에 PaaS 환경에도 나쁘지 않은 스토리지이다.
     
     그러나 ZFS는 결코 가볍지 않다. ZFS는 메모리를 상당히 소모한다. 즉, 도커에서 많은 수의 컨테이너를 동시에 사용해야한다면 호스트의 자원 사요량을 
     수시로 확인하는 것이 좋다. 
         
```