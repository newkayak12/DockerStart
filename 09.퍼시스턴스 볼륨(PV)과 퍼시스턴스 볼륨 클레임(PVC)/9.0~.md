```dockerfile
            퍼시스턴트 볼륨(PV)와 퍼시스턴트 볼륨 클레임(PVC)
    
    지금까지 사용했던 테스트용 디플로이먼트는 모두 상태가 없는(stateless) 애플리케이션이었다. 즉, 디플로이먼트의 각 포드는 별도의 데이터를 가지고 있지 
    않았으며, 단순히 요청에 대한 응답만을 반환했다. 그렇지만 데이터베이스처럼 포드 내부에서 특정 데이터를 보유해야하는, 상태가 있는(stateful) 애플리케이션
    의 경우에는 데이터를 어떻게 관리해야할지 고민해야 한다. 예를들어 MySQL 디플로이먼트를 통해 포드를 생성했다고 하더라도 MySQL 포드 내부에 저장된
    데이터는 절대 영속적이지 않다. 디플로이먼트가 삭제되면 포드도 함께 삭제되고, 그와 동시에 포드의 데이터 또한 함께 삭제된다. 
    
    이를 해경하기 위해서는 포드의 데이터를 영속적으로 저장하기 위한 방법이 필요하다. 이전에 도커에서 볼륨을 다룰 때 사용했던 'docker run -v' 옵션이나
    'docker volume' 명령어가 있다. 이러한 옵션들은 단일 컨테이너의 디렉토리를 호스트와 공유함으로써 데이터를 보존했다.
        
        {
            'docker volume create myVolume'
            'docker run -it --name test -v myVolume:/mn ubuntu:16.04'
        } 
        
    쿠버네티스에서도 호스트에 위치한 디렉토리를 각 포드와 공유함으로써 데이터를 보존하는 것이 가능하다. 그렇지만 여러 개의 서버로 구성된 쿠버네티스와 같은
    클러스터 환경에서는 이 방법이 적합하지 않을 수 있다. 쿠버네티스는 워커 노드 중 하나를 선택해서 포드를 할당하는데, 특정 노드에서만 데이터를 보관해 
    저장하면 포드가 다른 노드로 옮겨갔을 때 해당 데이터를 사용할 수 없게 된다. 따라서 특정 노드에서만 포드를 실행해야하는 경우가 발생할 수 있다.
    
    이를 해결할 수 있는 일반적인 방법은 어느 노드에서도 접근해서 사용할 수 있는 퍼시스턴트 볼륨(Persistent Volume)을 사용하는 것이다. 퍼시스턴트 볼륨은 
    워커 노드들이 네트워크상에서 스토리지를 마운트해 영속적으로 데이터를 저장할 수 있는 볼륨을 의미한다. 따라서 포드에 장애가 생겨 다른 노드로 옮겨가더라도
    해당 노드에서 퍼시스턴트 볼륨에 네트워크로 연결해 데이터를 계속해서 사용할 수 있다. 네트워크로 연결해 사용할 수 있는 퍼시스턴트 볼륨의 대표적인 에로는
    NFS, AWS의 EBS(Elastic Block Store), Ceph, GlusterFS 등이 있다.
    
    쿠버네티스는 퍼시스턴트 보륨을 사용하기 위한 기능을 자체적으로 제공하고 있다. 이번 장에서는 상태를 가지는 프도의 데이터를 보존하기 위한 쿠버네티스
    오브젝트인 퍼시스턴트 볼륨(Persistent Volume: PV), 퍼시스턴트 볼륨 클레임(Persistent Volume Clain: PVC)에 대해서 살펴볼 것이다.
    
    
    
            > 로컬 볼륨 : hostPath, emptyDir
    쿠버네티스가 어떻게 볼륨을 관리하는지 알아보기 전에 볼륨을 간단히 사용해 볼 수 있는 hostPath, emptyDir 두 가지 볼륨에 대해서 먼저 알아봐야한다.
    hostPath는 호스트와 볼륨을 공유하기 위해서 사용하고, emptyDir는 포드의 컨테이너 간에 볼륨을 공유하기 위해서 사용한다. 
    
          
          
            > 워커 노드의 로컬 디렉토리를 볼륨으로 사용하기 : hostPath
    포드의 데이터를 보존할 수 있는 가장 간단한 방법은 호스트의 디렉토리를 포드와 공유해 데이터를 저장하는 것이다. 호스트와 디렉토리를 공유하는 포드를
    생성하기 위해 아래의 내용으로 YAML 파일을 작성해보자
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-pod
spec:
  containers:
    - name: my-container
      image: busybox
      args: ["tail", "-f", "/dev/null"]
      volumeMounts:
      - name: my-hostpath-volume
        mountPath: /etc/data
  volumes:
    - name: my-hostpath-volume
      hostPath:
        path: /tmp
```
```dockerfile   
    이전에 컨피그맵을 포드의 볼륨으로 사용했던 것과 비슷한 형식이다. volumes 항목에 볼륨을 정의한 뒤, 이를 포드를 정의하는 conainers 항목에서 참조해서
    사용한다. 위 예시에서는 볼륨에서 hostPath 항목을 정의함으로써 호스트의 /tmp를 포드의 /etc/data에 마운트 했다.
    
    포드를 생성한뒤 포드의 컨테이너 내부로 들어가 /etc/data 디렉토리에 파일을 생성하면 호스트의 /tmp에 파일이 저장된다. 포드 컨테이너의 /etc/data와
    호스트 /tmp는 동일한 디렉토리로써 사용되는 것이다.
    
        'kubectl apply -f hostpath-pod.yaml'
        'kubectl exec -it hostpath-pod touch /etc/data/mydata'
        
    그렇지만 이러한 방식의 데이터 보존은 바람직하지 않다. 디플로이먼트의 포드에 장애가 겨 다른 노드로 포드가 옮겨갔을 경우, 이전 노드에 저장된 데이터를 
    사용할 수 없기 때문이다. hostPath 방식의 볼륨을 반드시 사용해야한다면 스케쥴링을 이용해 특정 노드에만 포드를 배치하는 방법도 생각해볼 수 있지만,
    이 방법 또한 호스트 서버에 장애가 생기면 데이터를 잃게 된다는 단점이 있다.
    
    그렇지만 hostPath 볼륨은 모든 노드에 배치해야하는 특수한 포드의 경우에 유용하게 사용할 수 있다. 예를 들어 모니터링 툴인 CAdvisor는 호스트의 디렉토리
    와 도커 소켓(/var/run/docker.sock)등을 컨테이너 내부로 공유해 모니터링 데이터를 수집했다.
    
        ' docker run \
        --volume=/:rootfs:ro \ 
        --volume=/var/run:/var/run:rw \
        --volume=/sys:/sys:ro \
        --volume=/var/lib/docker/:/var/lib/docker:ro \
    
    만약 CAdvisor와 같은 모니터링 툴을 쿠버네티스의 모든 워크 노드에 배포해야한다면 hostPath를 사용하는 것이 옳은 선택일 것이다. 단, 이러한 특수한 경우
    를 제외한다면 hostPath를 사용하는 것은 보안 및 활용성 측면에서 그다지 바람직하지 않으므로 hostPath를 사용하는 것을 신중히 고려하는 것이 좋다.
    
    
        > 포드 내의 컨테이너 간 임시 데이터 공유 : emptyDir
    emptyDir 볼륨은 포드의 데이터를 영속저긍로 보존하기 위해 외부 볼륨을 사용하는 것이 아닌, 포드가 실행되는 도중에만 필요한 휘발성 데이터를 각 컨테이너가
    함께 사용할 수 있도록 임시 저장 공간을 생성한다. emptyDir이라는 이름이 의미하는 것처럼 emptyDir 디렉토리는 비어있는 상태로 생성되며, 포드가 
    삭제되면 emptyDir에 있던 데이터도 함께 삭제된다. emptyDir을 사용하는 간단한 예시로 아파치 웹 서버를 생성하는 포드를 생성해볼 것이다. 아래의 yaml
    은 아파치 웹서버의 루트(htdocs)를 emptyDir에 마운트함과 동시에 이 디렉토리를 cotent-creator 컨테이너의 /data 디렉토리와 공유한다. 
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-pod
spec
  containers:
  - name: content-creator
    image: alicek106/alpine-wget:latest
    args: ["tail", "-f", "/dev/null"]
    volumeMounts:
    - name: my-emptydir-volume
      mountPath: /data      # 1. 이 컨테이너가 /data에 파일을 생성하면
      
  - name: apache-webserver
    image: httpd:2
    volumeMounts:
    - name: my-emptydir-volume
      mountPath: /usr/local/apache2/htdocs/  # 2. 아파치 웹서버에서 접근할 수 있다.

  volumes:
    - name: my-emptydir-volume
      emptyDir: {}              #포드 내에서 파일을 공유하는 emptyDir
```
```dockerfile
    emptyDir은 한 컨테이너가 파일을 관리하고 한 컨테이너가 그 파일을 사용하는 경우에 유용하게 사용할 수 있다. content-creator 컨테이너 내부로 들어가
    '/data' 디렉토리에 웹 콘텐츠를 생성하면 아파치 웹 서버 컨테이너의 htdocs 디렉토리에도 동일하게 웹 컨텐츠 파일이 생성될 것이고, 이는 최종적으로
    웹 서버에 의해 외부로 제공될 것이다. 
    
        'kubectl apply -f emptydir-pod.yaml'
        'kubectl exec -it emptydir-pod -c content-creater sh #echo Hello, Kubernetes! >> /data/test.html'

    이러한 emptyDir의 사용 방법은 단적인 예시일 뿐이며, 애플리케이션 구성에 따라 다양한 방법으로 사용할 수 있다. 예를 들어 깃허브 소스코드를 받아 empty
    Dir을 통해 애플리케이션 컨테이너에 공유해주는 사이드카 컨테이너를 생각해볼 수도 있고, 설정 파일을 동적으로 갱신하는 컨테이너를 포드에 포함시킬 수도 있다.
    
    
    
            > 네트워크 볼륨
    쿠버네티스에서는 별도의 플러그인 없이 다양한 종류의 네트워크 볼륨을 포드에 마운트할 수 있다. 온프레미스 환경에서도 구축할 수 있는 NFS, iSCSI, 
    GlusterFS, Ceph와 같은 네트워크 볼륨뿐만 아니라 AWS의 EBS(Elastic Block Store), GCP의 gcePersistentDisk와 같은 클라우드 플랫폼의
    볼륨을 포드에 마운트할 수도 있다. 
    네트워크 볼륨의 위치는 특별히 정해진 것이 없으며, 네트워크로 접근할 수만 있다면 쿠버네티스 클러스터 내부, 외부 어느 곳에 존재하도 크게 상관은 없다.
    단, AWS의 EBS와 같은 클라우드에 종속적인 볼륨을 사용하려면 AWS에서 쿠버네티스 클러스터를 생성할 때 특정 클라우드를 위한 옵션이 별도로 설정되어 있어야
    한다. 쿠버네티스에서 사용할 수 있는 네트워크 볼륨의 종류는 매우 많기 때문에 이번 절에서는 간단히 사용해볼 수 있는 NFS 볼륨의 사용법만 볼 것이다. 
    
            > NFS를 네트워크 볼륨으로 사용하기 
    NFS(Network File System)은 대부분의 운영체제에서 사용할 수 있는 네트워크 스토리지로 여러 개의 클라이언트가 동시에 마운트해 사용할 수 있다는 특징이
    있다. NFS는 여러 개의 스토리지를 클러스터링하는 다른 솔루션에 비해서 안정성이 떨어질 수 있으나 하나의 서버만으로 간편하게 사용할 수 있으며, NFS를
    마치 로컬 스토리지처럼 사용할 수 있다는 장점이 있다. NFS를 사용하려면 NFS 서버와 NFS 클라이언트가 각각 필요하다. NFS 서버는 영속적인 데이터가
    실제로 저장되는 네트워크 스토리지 서버이고, NFS 클라이언트는 NFS 서버에 마운트해 스토리지에 파일을 읽고 쓰는 역할을 한다. NFS 클라이언트는 워크 노드
    의 기능을 사용할 것이므로 따로 준비할 필요는 없으며, NFS 서버만 별도로 구축하면 된다.
    
    실제 운영환경에서 NFS를 사용하려면 별도의 튜닝이 적용된 NFS 서버를 구축하는 것이 좋지만 테스트로 쿠버네티스 클러스터 내부에 임시 NFS 서버를 생성해보자
    아래의 내용으로 nfs-deployment.yaml파일과 nfs-service.yaml을 작성한 뒤 디플로이먼트를 생성한다.
```
```yaml
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: nfs-server
spec:
  selector:
    matchLabels:
      role: nfs-server
  template:
    metadata:
      labels:
        role: nfs-server
    spec:
      containers:
      - name: nfs-server
        image: gcr.io/google_containers/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
```