```dockerfile
                > 애플리케이션 배포를 위한 고급 설정 
            1. 포드의 자원 사용량 제한
    쿠버네티스와 같은 컨테이너 오케스트레이션 툴의 가장 큰 장점 중 하나는 여러 대의 서버를 묶어서 리소스 풀로 사용할 수 있다는 것이다. 클러스터의 CPU나
    메모리 등의 자원이 부족할 떄 필요한 용량만큼 서버를 동적으로 추가함으로써 수평적으로 확장할 수 있기 떄문이다. 하지만 서버를 수평적으로 늘리는 스케일 아웃
    (scale-out) 만큼 중요한 작업이 하나 이다. 바로 클러스터 내부에서 컴퓨팅 자원 활용률(Utilization)을 늘리는 것이다.
    
    자원 활용률은 서버 클러스터에서 자원을 얼마나 효율적으로 빠짐없이 사용하고 있는지를 의미한다. 예를 들어 쿠버네티스에서 실행 중인 컨테이너의 CPU, 메모리
    사용량이 현저히 낮거나 유휴 상태의 컨테이너에게 과잉 할당을 했다면 이를 자원 활용률이 낮다고 표현한다. 이러한 상황을 방지하기 위해서 각 컨테이너에
    적절한 자원 사용량을 제한해야하며, 남는 자원을 사용할 수 있는 전략을 세워야 한다. 
    
    쿠버네티스는 컴퓨팅 자원을 컨테이너에 할당하기 위한 여러 기능을 제공한다. 이번 장에서는 포드나 컨테이너에 CPU, 메모리 등의 자원을 할당하는 기본 방법을
    먼저 알아본 후, 쿠버네티스의 자원 활용률을 높이기 위한 오버커밋(Overcommit) 방법을 알아본다. 그 후 ResourceQuota, LimitRange라는 쿠버네티스 
    오브젝트를 알아본다.
    
            > 1.1 컨테이너와 포드의 자원 사용량 제한 : Limits
    쿠버네티스에서 자원을 할당하는 방법을 알아보기 전 도커 컨테이너의 자원 제한을 다시 상기시켜보자. 컨테이너의 자원 사용량을 제한하는 방법은 여러 가지가 있지만
    '--memory', '--cpu', '--cpu-shares', '--cpu-quota' 및 '--cpu-runtime' 등이 있다. '--memory' 옵션은 컨테이너가 사용 가능한 최대
    메모리 사용량을 제한하며, 그외의 옵션들의 CPU의 사용량을 제한한다. 이 중에서 '--cpu-shares'는 정량적인 CPU 할당량이 아닌 비율 값을 사용한다는
    점에서 특별한 옵션이었다. 
    
        'docker run -it --name memory_1gb --memory 1g ubuntu:16.04'
        'docker run -it --name cpu_1_alloc --cpus 1 ubuntu:16.04'
        'docker run -it --name cpu_shares_example --cpu-shares 1024 ubuntu:16.04'
    
    또는 다음관 같이 자원 제한 옵션을 설정하기 않고 도커 컨테이너를 생성하면 호스트의 모든 CPU, 메모리를 사용할 수 있었다.
    
        'docker run -it --name unlimited_blade unbuntu:16.04'
    
    쿠버네티스는 기본적으로 도커를 컨테이너 런타임으로 사용하기 때문에 포드를 생성할 때 docker와 동일한 원리로 CPu, 메모리 최대 사용량을 제한할 수 있다.
    그렇지만 지금까지 포드나 디플로이먼트 등을 생성할 때 자원 할당량을 명시적으로 제한했던 적이 없다는 것을 눈치챘을 것이다. 이처럼 자원 할당량을 설정하지 
    않으면 포드의 컨테이너가 노드의 물리 자원을 모두 사용할 수 있기 때문에 노드의 자원이 모두 고갈되는 상황이 발생할 수 있다. 
    
    이 예방할 수 있는 가장 간단한 방법은 포드 자체에 자원 사용량을 명시적으로 설정하는 것이다. 포드의 CPU, 메모리 사용량을 제한하기 위해 아래 내용으로 
    YAML을 작성해보자.
```
resource-limit-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-limit-pod
  labels:
    name: resource-limit-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
  resources:
    limits:
      memory: '256Mi'
      cpu: '1000m'
```
```dockerfile
    포드를 정의하는 스펙에 새롭게 spec.containers.resources.limits 항목을 정의했다. 메모리는 256를 입력했는데, 이 설정값은 도커 명령어에서
    'docker run --memory 256m'과 같다. 즉 이 포드의 컨테이너 최대 메모리 사용량은 256Mi으로 제한된다. 
    cpu에는 1개의 cpu를 뜻하는 1000m(밀리코어)라는 값을 입력했으며, 이는 도커 명령에서 'docker run --cpus 1'와 같다. 따라서 이 포드의 컨테이너는
    최대 1개 CPU 만큼의 사이클을 사용할 수 있다. 
    
        {
            이전에 포드에는 여러 개의 컨테이너가 존재할 수 있다. 따라서 하나의 포드에 여러 개의 컨테이너가 할당돼 있다면 각 컨테이너에 대해
            자원을 각각 할당할 수 있다. 
        }
        
    위 내용으로 YAML을 작성한 후 포드를 생성하면 'kubectl apply -f resource-limit-pod.yaml'
    
    'docker info' 명령어로 도커 호스트의 가용 자원을 확인할 수 있던 것처럼 쿠버네티스에서도 'kubectl describe node' 명령어로 워커 노드의 가용 자원을
    간단하게 확인할 수 있다. 이를 위해 방금 생성한 포드가 어느 노드에 생성돼 있는지 먼저 확인한 다음, 'kubectl describe' 명령어로 해당 노드의 자세한
    정보를 확인한다.
    
        'kubectl get pods -o wide'
        'kubectl descrbie node [NODE....]'
        
    'kubectl describe node' 명령어의 출력 내용 중에서 중간 부분에 위치한 Non-terminated Pods 항목에서는 해당 노드에서 실행 중인 포드들의 자원
    할당량을 확인할 수 있다. 방금 생성한 resource-limit-pod라는 이름의 포드 외에도 kube-system 네임스페이스의 포드가 미리 존재하고 있을 것이다.
    이 포드들은 쿠버네티스의 네트워크를 위한 핵심 컴포넌트로, 따로 설정하지 않아도 기본적으로 CPU와 메모리가 할당된다.
    
    'kubectl describe node' 명령어의 출력 내용 중에서 중간 부분에 위치한 Non-terminated Pods 항목에서는 해당 노드에서 실행 중인 포드들의 자원
    할당량을 확인할 수 있다. 방금 생성한 resource-limit-pod라는 이름의 포드 외에도 kube-system 네임스페이스의 포드가 미리 존재하고 있을 것이다.
    이 포드들은 쿠버네티스의 네트워크를 위한 핵심 컴포넌트로, 따로 설정하지 않아도 기본적으로 CPU와 메모리가 할당된다.
    
    그 아래 Allocated resources 항목에서는 해당 노드에서 실행 중인 포드들의 자원 할당량을 모두 더한 값이 출력된다. 즉, 방금 생성한 resource-limit-pod
    라는 이름의 포드 및 다른 시스템 컴포넌트의 자원 할당량의 합계를 확인할 수 있다. 
    
    
    
            > 1.2 컨테이너와 포드의 자원 사용량 제한하기 : Request
    limits는 해당 포드의 컨테이너가 최대로 사용할 수 있는 자원의 상한선을 의미한다. 방금 생성한 포드 또한 YAML 파일에서 Limits를 설정했고, 포드의 컨테이너는
    Limits보다 더 많은 자원을 사용할 수 없다는 것을 알 수 있다. 여기서 Request라는 단어가 등장했다. 
    
    쿠버네티스의 자원 관리에서 Request는 '적어도 이 만큼의 자원은 컨테이너에게 보장돼야 한다.(하한선)'는 의미이다. Limits의 개념과 유사해보이지만, Requests는
    쿠버네티스에서 자원의 Overcommit을 가능하게 하는 개념이다. 
    Request가 정확히 어떤 기능을 하는지 설명하기 전에 자원의 오버커밋이 무엇인지 알아야 한다. 오버커밋은 한정된 컴퓨팅 자원을 효율적으로 사용하기 위한 방법으로,
    사용할 수 있는 자원보다 더 많은 양을 가상 머신이나 컨테이너에게 할당함으로써 전체 자원의 사용률(Utilization)을 높이는 방법이다. 
    
    1GB의 전체 메모리에서 A -> 512MB, B -> 512MB를 정적으로 할당하는 것은 비효율이 생긴다. 만약 사용률이 낮은 컨테이너가 높은 컨테이너에게 빌려줄 수 
    있다면 좋을 것이다. 이처럼 정적인 할당은 유휴 자원을 제대로 활용하지 못한다. 이러한 문제를 해결하는 가장 좋은 방법은 사용량을 예측하여 적절히 결정 하는
    것이지만, 컨테이너가 실제로 얼마나 사용할지 예측하기란 여간 어려운 일이다. 
    
    이를 위해서 쿠버네티스에서는 오버커밋을 통해 물리 자원보다 더 많은 양의 자원을 할 당할 수 있다. 1GB 전체 메모리라도 750MB, 750MB인 포드 두 개를 
    생성할 수 있다고 보면 된다. 실제 메모리가 1GB이므로 실제로는 그렇게 되지 않지만 A가 사용률이 낮다면 B에 빌려주는 식으로 자원을 효율적으로 사용하게 할 수 
    있다. 이러한 자원 제한을 쿠버네티스에서는 Limits라고 한다. 위에서 작성한 resource-limit-pod의 resources가 이러한 설정이다.
    그렇지만 만약 A가 500MB를 사용할 때, B가 750MB를 사용하고자 하면 어떻게 될까? 이러한 상황을 방지하기 위해서 새로운 개념이 도입된다. 각 컨테이너가
    '사용을 보장받을 수 있는 경계선'을 정하는 것이다. A와 B 컨테이너에 하한선을 512MB로 잡았지만 Limits은 750MB로 같다. 두 컨테이너 모두 메모리 사용을
    보장받을 수 있는 경계선이 500MB이기 떄문에 500MB 이내에 메모리를 사용한다면 문제가 되지 않는다.
    
    그렇지만 컨테이너 A가 500MB를 사용하고 있을 때 컨테이너 B가 750MB를 사용하려면 B는 메모리 사용에 실패한다. 컨테이너 A는 메모리 사용을 보장받을 수 
    있는 경계선 내에 500MB를 사용하고 있는데, 컨테이너 B가 이를 침범하려 했기 때문이다. 이러한 경계선을 쿠버네티스에서 Request라고 한다. 즉, Request는
    컨테이너가 최소한으로 보장받아야 하는 자원의 양을 뜻한다. 
    
    포드에 설정될 Request 자원의 값은 Limit과 같이 YAML에서 지정할 수 있다.
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-limit-with-request-pod
  labels:
    name: resource-limit-with-request-pod
spec:
  containers:
    - name: nginx
      image: nginx:latest
      resources:
        limits:
          memory: "256Mi"
          cpu: "1000m"
        requests:
          memory: "128Mi"
          cpu: "500m"
```
```dockerfile
    먼저 이 YAML 파일에 정의된 포드 컨테이너의 메모리 할당량을 보자 requests에서 128Mi를 limit을 256Mi로 설정했기에 '하한선 128mb 유휴 자원 존재 시
    256mb 까지 설정한다. '는 의미를 갖는다. CPU 또한 같은 원리로 해석하면 '하한선 0.5CPU ~ 상한선 1CPU' 이다.
    단, requests는 컨테이너가 보장받아야 하는 최소한의 자원을 뜻하기 때문에 노드의 총 자원의 크기보다 더 많은 양을 할당할 수는 없다. 따라서 쿠버네티스
    스케쥴러는 포드의 request만큼 여유가 있는 노드를 선택해 포드를 생성한다. 즉, 포드를 할당할 떄 사용되는 자원 할당 기준은 Limit이 아닌 Request이다.
    Requests의 값을 낮게 설정해서 포드를 생성하면 포드가 스케쥴링 되어 노드에 할당될 확률은 높아지겠지만, 포드가 사용을 보장받을 수 있는 자원의 양은 적어질 것이다.
    
        {
            노드에 할당된 포드의 Limits 값의 합은 노드의 물리 자원 크기를 초과할 수도 있다. 
        } 
        
            
            
            > 1.3 CPU 자원 사용량의 제한 원리
            쿠버네티스에서 CPU Requests와 Limits
    앞서 설명한 것처럼 쿠버네티스에서는 CPU를 m(밀리코어) 단위로 제한하며, 1개의 CPU는 1000m에 해당한다. 따라서 서버에 2개의 CPU가 존재한다면 최대
    2000m 만큼의 CPU Requests를 포드의 컨테이너에 할당할 수 있다.
    
    기본적인 CPU 자원의 단위를 이해했다면 이번에는 포드의 CPU Request와 Limit가 실제로 어떻게 동작하는지 알아보자. 이전에 사용했던 포드의 YAML 파일에는
    CPU의 Limits와 Requests가 각각 설정돼 있다. 
```
resource-limit-with-request-pod.yaml
```yaml
...
  resources:
    limits:
      memory: "256Mi"
      cpu: "1000m"      #최대 1개 CPU만큼 사용할 수 있다.
    requests:
      memory: "128Mi"
      cpu: "500m"       #최소한 0.5개의 CPU만큼 사용을 보장받을 수 있다.
```
```dockerfile
    CPU의 Limits를 의미하는 resources.limit.cpu를 1000m 설정하면 포드의 컨테이너는 최대 1개 만큼의 CPU를 사용할 수 있다. 즉, 이는 아래의 도커
    명령과 같다. 이는 컨테이너가 사용 가능한 CPU의 최대한도를 나타내기 때문에 어렵지 않게 이해할 수 있다. 
        {
            'docker run --cpus 1 ...'
            'docker run --cpu-period 100000 --cpu-runtime 100000....'
        }
    그렇다면 resources.requests.cpu를 설정해 CPU가 보장받아야하는 최소한의 CPU 자원을 설정하면 컨테이너에서 어떻게 설정될까? 결론부터 말하면 CPU의
    Requests는 docker run의 '--cpu-shares' 옵션과 같다. '--cpu-shares'는 서버에 CPU가 실제로 몇 개가 있는지에 상관없이 '--cpu-shares'의 할당
    비율에 따라 컨테이너가 사용할 수 있는 CPU 자원이 결정되는 옵션이다. '--cpu-shares'가 설정된 여러 개의 컨테이너가 동시에 존재한다면 각 컨테이너의
    '--cpu-shares' 비율에 따라 CPU를 사용할 수 있다. 예를 들어 3개의 컨테이너에서 '--cpu-shares'를 각각 1024, 1024, 512로 설정했다면 CPU의 
    사용률이 100%가 되는 포화 상태에서는 각 컨테이너가 2:2:1의 비율으로 CPU를 사용할 수 있을 것이다. 단, '--cpu-shares'을 얼마나 설정했는지와는 상관없이
    시스템에 CPU의 유휴 자원이 존재할 때는 CPU를 전부 사용할 수 있다. 
    
    도커 명령어를 공부할 떄는 '--cpu-shares' 옵션이 쓸모없어 보였을 수도 있지만, 이 옵션을 '--cpus(Limits)'와 함께하면 CPU 자원에 오버 커밋을 적용
    할 수 있다. 다시 쿠버네티스로 돌아와서 이번에는 '--cpu-shares(Requests)'와 '--cpus(Limits)'가 함께 설정된 컨테이너를 생각해보자
    
        {
            쿠버네티스는 CPU 단위를 밀리코어(m) 단위로 나타내기 때문에 Requests('--cpu-shares')의 값 또한 m으로 나타낸다. 이 값이 실제로 CPU
            Share의 값으로 변환되는 식은 아래와 같다.
                (Requests에 설정된 CPU 밀리 코어 값 * 1024) / 1000 = CPU Share의 값
                ex) (500m * 1024) / 1000 = 512 (실제로 컨테이너에 설정된 '--cpu-shares의 값')
        }
    
    컨테이너가 하나만 존재하는 상황이라면 Requests('--cpu-shares')와 상관없이 CPU의 Limits의 값(700m)만큼 사용할 수 있을 것이다. 그렇지만
    두 개의 컨테이너 Limit은 700m, requests는 500m이라면 어떨까?
    
    두 컨테이너의 Requests('--cpu-shares') 비율은 500m으로 같기 떄문에 CPU 자원 포화일 경우, 1:1 비율로 사용할 것이다. 따라서 Requests의 값인
    500m은 최종적으로 각 컨테이너에 보장돼야 하는 최소한의 CPU 자원을 나타낸다고 할 수 있다. 전체 CPU 자원인 1000m 만큼 Requests를 소진했기에
    새로운 Requests('--cpu-shares')를 가지는 컨테이너를 새롭게 할당하는 것은 불가능하다. 
    이번에는 Requests보다 더 많은 CPU자원을 사용하려 할 때, 자원의 경합(Contention)이 발생하는 상황을 가정해보자. 다른 컨테이너가 CPU를 사용하고 있지
    않아 유휴 CPU 자원이 발생한다면 다른 컨테이너는 Limits에 설정된 CPU 값만큼 사용할 수 있다.
    
    총 1000m 인 CPU에서 컨테이너 A가 Requests보다 더 많은 CPU를 사용하고 있는데(700), 컨테이너 B가 Requests 만큼 CPU를 사용하려고 하면(500) 
    컨테이너 A에는 CPU 쓰로틀(throttle)이 발생한다. 따라서 컨테이너 B는 Requests에 명시한 만큼의 CPU 비율을 사용할 수 있다. 
    
        {
            쿠버네티스에서는 CPU를 압축 가능한 (Compressible) 리소스라고 부른다. 이는 Requests보다 더 많은 CPU를 사용해서 CPU 경합이 발생하더라도
            컨테이너의 CPU 사용량을 쓰로틀을 사용해서 억제할 수 있기 때문이다. 이와 반대로 메모리나 스토리지는 압축 불가능한(Incompressible) 리소스
            라고 부르는데, 컨테이너 간에 메모리 사용의 경합이 발생하면 우선순위가 낮은 컨테이너의 프로세스가 먼저 종료되기 때문이다. 
        }
        
    아래와 같이 Requests에 할당되지 않은 여유 CPU 자원이 노드에 남아있는 경우도 생각해 볼 수 있다. 앞에서 서술한 바와 같이 Limits만큼 CPU를 사용할 수 
    있다면 컨테이너는 Limits까지 CPU를 사용할 수 있다. 하지만 두 컨테이너가 동시에 최대치를 사용하려 하면 Requests('--cpu-shares')의 비율에
    맞춰서 사용하도록 한다. 
    
        
        
                > 1.4 QoS 클래스와 메모리 자원 사용량 제한 관리
    지금까지 설명한 내용을 다시 보면 포드의 컨테이너는 최대 Limits만큼의 자원을 사용할 수 있지만, 최소한 Requests 만큼의 자원을 사용할 수 있도록 보장
    받는다. 이떄 Requests보다 더 많은 자원을 사용하는 것을 오버커밋이라고 부르며, Requests를 넘어서 자원을 사용하려 시도하다 다른 컨테이너와 자원이
    충돌하게 되면 CPU 스로틀과 같은 원리에 의해 자원 사용이 실패할 수 있다. CPU의 사용량에 경합이 발생하면 일시적으로 컨테이너 내부의 프로세스에 CPU
    쓰로틀이 걸릴 뿐, 컨테이너 자체에는 큰 문제가 발생하지 않는다.
    
    그렇지만 메모리의 사용량에 경합이 발생하면 문제가 심각해진다. 앞서 설명했던 것처럼 프로세스의 메로리는 이미 데이터가 메모리에 적재돼 있기 때문에 CPU와
    달리 메모리는 압축 불가능(Incompressible) 자원으로 취급된다. 따라서 하나의 노드에서 여러 개의 컨테이너가 Requests보다. 더 많은 자원을 사용하려고
    시도해도 이미 메모리에 적재된 데이터를 압축할 수는 없다. 이러한 상황에 쿠버네티스는 가용 메모리를 확보하기 위해서 우선순위가 낮은 포드 또는 프로세스를 
    강제로 종료하도록 설계되어 있다. 강제로 종료된 포드는 다른 노드로 옮겨가는데 이를 쿠버네티스에서는 퇴거(Eviction)이라고 한다. 
    
    그렇다면 여기서 가장 중요한 부분이 '노드에 메모리 자원이 부족해지면 어떤 포드나 프로세스가 먼저 종료돼야 하는가?'이다. 이를 위해 쿠버네티스는 포드의 
    컨테이너에 설정된 Limtis와 Requests 값에 따라 내부적으로 우선순위를 계산한다. 그 뿐만 아니라 쿠버네티스는 포드의 우선순위를 구분하기 위해서 QoS
    (Quality Of Service) 클래스를 명시적으로 포드에 설정한다.
    
            > 쿠버네티스에서의 메모리 부족과 OOM(Out Of Memory)
    쿠버네티스의 노드에는 각종 노드의 이상 상태 정보를 의미하는 Condtions라는 값이 존재합니다. 이 값은 kubectl describe nodes 명령어로도 확인할 
    수 있다. 이상 상태의 종류에서는 MemoryPressure, DiskPressure 등이 있다. 쿠버네티스의 에이전트인 kubeletdms 노드의 자원 상태를 주기적으로
    확인하면서 Conditions의 MemoryPressure, DiskPressure 등의 값을 갱신한다. 
    
    예를 들어, 평소에 메모리가 부족하지 않을 때는 MemoryPressure의 값이 False로 설정되어 있다가 노드의 가용 메모리가 부족해지만 MemoryPressure
    상태의 값이 True가 된다. MemoryPressure는 기본적으로 노드의 가용 메모리가 100Mi 이하일 때 발생하도록 설정돼 있다. MemoryPressure가 발생하면
    쿠버네티스는 해당 노드에 실행 중이던 모든 포드에 대해서 순위를 매긴 다음, 가장 우선순위가 낮은 포드를 다른 노드로 퇴거(Evict) 시킨다. 그 뿐만 아니라
    MemoryPressure의 값이 True인 노드에는 더 이상 포드를 할당하지 않는다. 이때 포드의 우선순위는 QoS 클래스 및 메모리 사용량에 따라 정렬된다. 
    
        {
            MemoryPressure와 같은 이상 상태를 감지하기 위한 임계치를 Hard Eviction Threshold라고 부르며, kubelet의 실행 옵션에서 설정 값을
            적절히 변경할 수도 있다. Hard Eviction Threshold의 다른 예시로 DiskPressure가 있으며, DiskPressure의 상태가 활성화(True)되면
            쿠버네티스는 사용 중이지 않은 도커 이미지를 자동으로 삭제하기도 한다.
        }
    
    만약 kubelet이 MemoryPressure 상태를 감시하기 전에 급작스럽게 메모리 사용량이 많아질 경우, 리눅스 시스템의 OOM(Out Of Memory) Killer라는
    기능이 우선순위가 낮은 컨테이너의 프로세스를 강제로 종료해서 가용 메모리를 확보할 수도 있다. OOM의 우선 순위 점수에는 oom_score_adj이고, 두 번째는
    oom_score이다. OOM Killer는 oom_score의 값에 따라서 종료할 프로세스를 산정한다. 
    
    OOM Killer는 리눅스에 기본적으로 내장된 기능이기 때문에 아무런 설정 없이 모든 프로세스에 자동으로 OOM 점수를 매긴다. OOM 점수가 높으면 높을수록
    강제 종료 가능성이 커지므로 절대로 종료되지 말아야 하는 핵심 프로세스는 일반적으로 매우 낮은 값을 부여 받는다. 
    
    예를 들어 쿠버네티스를 설치함으로써 실행되는 도커 데몬은 기본적으로  OOM 점수가 -999이다. 이 점수는 쿠버네티스 노드 중 하나에 접속하면 확인할 수 있다.
    이러한 프로세스는 메모리 부족이 생겨도 강제 종료 되는 일은 거의 없다. 
    
        'ps aux | grep dockerd'
        'ls /proc/[PID]' -> oom_adj, oom_score, oom_score_adj
        'cat /proc/[PID]/oom_score_adj' -> -999
        
    프로세스가 메모리를 얼마나 사용하고 있는지에 따라 프로세스의 최종 OOM 점수(oom_score)가 갱신되는데, OOM Killer는 이 점수를 기반으로 최종적으로
    종료할 프로세스를 결정한다. 
    
    
            > QoS 클래스의 종류 (1) Guaranteed 클래스 
    쿠버네티스에서는 포드의 Limits와 Requests 값에 따라서 'QoS 클래스'라는 것을 모든 포드에 대해서 설정한다. QoS 클래스에는 BestEffort, Burstable,
    Guaranteed 총 세 종류가 있다. QoS 클래스는 우리가 설정하지 않아도 자동으로 설정되므로 kubectl describe 명령어로 포드 정보를 조회하면
    QoS 클래스를 확인할 수 있다. 
    
        {
            'kubectl describe pod resource-limit-pod | grep QoS'
            QoS Class: Guaranteed
        }
    
    resource-limit-pod라느 이름의 포드는 Guaranteed라는 QoS 클래스로 설정됐다. Guaranteed 클래스는 포드의 컨테이너에 설정된 Limits와 Requests
    값이 완전히 동일할 떄 부여되는 클래스이다. 이전에 위의 resource-limit-pod 포드를 생성할 떄 사용했던 YAML 파일을 다시 살펴보자.
```
```yaml
...
containers:
  - name: nginx
    image: nginx:latest
    resources:
      limits:
        memory: '256Mi'
        cpu: '1000m'
```
```dockerfile
    이전에 생성할 때 limits만 명시했는데 Guaranteed 포드로 분류됐다 이는 Requests 없이 Limit만 정의하면 Requests가 Limits과 동일하게 설정되기
    때문이다. 혹은 아예 Requests, Limits을 동일하게 명시해도 Guaranteed로 생성된다. Guaranteed 클래스로 분류되는 포드는 Requests와 Limits
    값이 동일하기 때문에 할당받은 자원을 아무런 문제 없이 사용할 수 있다. 물론 Requests의 범위 내에서도 자유롭게 자원을 사용할 수 있으며, Requests의
    값이 Limits와 동일하기 때문에 Requests보다 더 많은 자원을 사용하지도 않는다. 즉, 자원의 오버커밋이 허용되지 않기 때문에 할당받은 자원의 사용을
    안정적으로 보장(Guaranteed) 받을 수 있다고 생각하면된다. 
    Guaranteed 클래스의 포드 내부에서 실행되는 프로세스들은 모두 기본  OOM 점수(oom_score_adj)가 -998로 설정된다. 도커 데몬이나 kubelet의 프로
    세스가 거의 동일한 레벨로 프로세스가 보호 받기에 노드에서 메모리가 고갈되어도 시스템 컴포넌트가 요구하지 않는 한 Guaranteed 클래스의 포드나 프로세스가
    강제로 종료되는 일은 없다. 
    
            > QoS 클래스의 종류 (2) BestEffort 클래스 
    BestEffort는 Requests와 Limits를 아예 설정하지 않는 포드에 설정되는 클래스이다. 즉, 포드의 스펙을 정의하는 YAML 파일에서 resources 항목을
    아예 사용하지 않으면 BestEffort 클래스로 분류된다. 
```
```dockerfile
apiVersion: v1
kind: Pod
metadata:
    name:nginx-besteffort-pod
spec:
    containers:
    - name: nginx-besteffort-pod
      image: nginx:latest
```
```dockerfile
    BestEffor 클래스의 포드는 Limits 값을 설정하지 않았기에 노드에 유휴 자원이 있다면 제한 없이 모든 자원을 사용할 수 있다 그러나 Requests 또한
    설정하지 않았기 때문에 BestEffort 클래스의 포드는 사용을 보장받을 수 있는 자원이 존재하지 않는다. 따라서 떄에 따라서는 노드에 존재하는 모든 자원을
    사용할 수도 있지만 자원을 전혀 사용하지 못할 수 도 있다.
    
            
            > QoS 클래스 (3) Burstable 클래스 
    Burstable 클래스는 Requests와 Limits가 설정돼 있지만, Limits의 값이 Requests보다 큰 포드를 의미하낟. 따라서 Burstable 클래스의 포드는
    Requests에 지정된 자원만큼 사용을 보장받을 수 있지만 상황에 따라서는 Limits까지 자원을 사용할 수 있다. Burstable이라는 이름이 의미하는 것처럼
    필요에 따라서 순간적으로 자원의 한계를 확장해 사용할 수 있는 포드라고 생각하면 된다.
    
    Guaranteed, BestEffor에 속하지 않는 포드는 모두 Burstable이라고 보면 된다. 아래와 같은 예시가 Burstable이다. 
```
```yaml
    ...
    resources:
      limits:
        memory: '256Mi'
        cpu: '1000m'
      requests:
        memory: '128Mi'
        cpu: '500m'
```
```dockerfile
    Burstable 클래스의 포드는 주어진 Requests 내에서 자원을 사용하면 문제가 없지만, Requests를 넘어 Limits 범위 내에서 자원을 사용하려 시도한다면
    다른 포드와 자원 경합이 발생할 수도 있다. 그러한 상황에서 Requests보다 더 많은 자원을 사용하고 있는 포드나 프로세스의 우선순위가 더 낮게 설정된다.
    
    
                > QoS 클래스와 메모리 부족
    일전에 서술한 바와 같이 kubelet이 메모리가 부족한 상황을 감지하면 우선순위가 가장 낮은 포드를 종료한 뒤 다른 노드로 내쫗는 퇴거(Evict)를 수행한다. 
    만약 메모리 사용량이 값작스럽게 높아지면 리눅스의 OOM Killer는 OOM 점수가 가장 낮은 컨테이너의 프로세스를 강제로 종료한 수도 있다. 포드가 다른 노드로
    퇴거(Evict)되면 단순히 다른 노드에서 포드가 다시 생성될 뿐이지만, OOM Killer에 의해서 종료되면 포드의 재시작 정책(restartPolicy)에 따라서 다시
    시작된다. 
    
            {
                OOM Killer는 메모리를 많이 사용하는 프로세스를 강제 종료하는 것이다. 컨테이너나 포드를 종료하는 것은 아니다. 따라서 컨테이너 내부의 
                init 프로세스가 아닌 다른 프로세스가 메모리를 많이 사용하고 있다면 해당 프로세스만 종료될 수 있다.
            }
    기본적으로 포드의 우선 순위는 Guaranteed가 가장 높으며, 그 뒤로 Burstable과 BestEffort 따라서 노드에 메모리가 부족하면 가장 먼저 BestEffort
    포드가 종료되고, 그 다음 Burstable이 종료되는 것이 일반적이다. Guaranteed 포드는 낮은 우선순위의 포드가 존재하지 않을 떄 마지막으로 종료된다.
    
    하지만 이러한 우선순위는 항상 절대적인 것은 아니다. 왜냐하면 Burstable과 BestEffort 클래스의 포드는 현재 메모리를 얼마나 사용하고 있는지에 따라서 
    우선 순위가 역전될 수도 있기 때문이다. 정확히 말하면 포드가 메모리를 많이 사용하면 사용할 수록 우선순위가 낮아진다. 예를 들어, Guaranteed 포드
    내부의 프로세스는 메모리 사용을 보장받아야 하기 때문에 우선순위가 가장 높지만, Burstable과 BestEffort 포드의 프로세스는 메모리를 많이 사용할수록
    우선순위가 낮아진다.
    
    - 자원 오버커밋의 필요성
    쿠버네티스에서 사용할 수 있는 자원 오버커밋의 원리는 때로는 복잡할 수 있다. 포드가 언제, 어떻게, 얼마나 자원을 사용하게 될지를 정확히 예측하기는 어려우며,
    오버커밋을 허용함으로 인해 예상치 못한 문제가 발생할 수도 있기 때문이다. 
    
    오버커밋이 쿠버네티스의 기능이라고 해서 반드시 사용해야만 하는 것은 아니다. 오히려 애플리케이션이 반드시 안정적인 상태로 동작해야한다면
    모든 포드의 Limits, Requests를 동일하게 설정함으로 Guaranteed 클래스를 생성하는 것도 해답이 된다. 
    
    
            > 1.5 ResourceQuota와 LimitRange
    여러 개발팀이 쿠버네티스에서 작업한다면 어떻게 환경을 구성하는 것이 좋을까? 개발팀마다 쿠버네티스를 구축하는 것이 좋지만 관리가 번거롭고 클러스터 프로비저닝
    을 위한 비용도 높아진다.
    이를 위해서 네임스페이스를 생성하여 각 개발팀이 해당 네임스페이스에서만 API를 사용할 수 있도록 롤, 롤바인딩으로 RBAC(Role-Base Access Control)
    을 구성하는 방법을 고려하는 것이 좋다. 그렇지만 한 쪽에서 자원을 독점한다면 다른 네임스페이스의 자원이 부족한 상황이 연출된다.  이를 위해서 네임스페이스
    당 자원 한도를 정하는 것이 좋다. 이를 위해 쿠버네티스에서는 ResourceQuota와 LimitRange라는 오브젝트를 이용해서 자원량을 관리할 수 있다.
    
    
            > 1.5.1 ResourceQuota로 자원 사용량 제한
    ResourceQuota는 특정 네임스페이스에서 사용할 수 있는 자원 사용량의 합을 제한할 수 있는 쿠버네티스 오브젝트이다. 
    
        1. 네임스페이스에서 할당할 수 있는 자원 (CPU, 메모리, 퍼시스턴트 볼륨 클레임 사이즈, 컨테이너 내부의 임시 스토리지)의 총량을 제한할 수 있다.
        2. 네임스페이스에서 생성할 수 있는 리소스(서비스, 디플러이먼트 등)을 제한할 수 있다.
        
    1번은 네임스페이스 별 자원 소모량을 제한하기 위해서이다. 2번 역시 쿠버네티스 클러스터의 자원 고갈을 막기 윔이다. 예시로 관리자가 실수로 쿠버네티스에서
    리소스를 무한정 생성한다고 하면 쿠버네티스 리소스의 개수에 제한이 없다면 메모리, 스토리지가 가득 찰 때까지 리소스를 생성할 것이고 결국 쿠버네티스가 정상적으로
    작동하지 못할 수 있다. ResourceQuota는 이러한 상황을 방지하기 위해서 네임스페이스에서 생성할 수 있는 서비스, 디플로이먼트 컨피그맵 등의 리소스 개수를
    제한하는 기능을 제공한다.
    
    ResourceQuota는 네임스페이스에 종속되는 오브젝트이므로 네임스페이스별로 ResourceQuota를 생성해야한다. 기본값은 어떠한 ResourceQuota도 생성되어
    있지 않다.
    
            'kubectl get quota |(resourcequota)' #quota라는 이름으로도 사용 가능
    
    ResourceQuota는 퍼시스턴트 볼륨 클레임이나 ephemeral-storage의 크기를 제한하기 위해서 사용할 수도 있으나, 이번 절에서는 default 네임스페이스의
    CPU, 메모리의 Request, Limits를 제한하는 것을 해볼 것이다.
```
resource-quota.yaml
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-quota-example
  namespace: default
spec:
  hard:
  request.cpu: '1000m'
  request.memory: '500Mi'
  limits.cpu: "1500m"
  limit.memory: '1000Mi'
```
```dockerfile
    반드시 Requests, Limits을 함께 제한할 필요는 없으며 CPU, 메모리를 하나의 ResourceQuoata에서 제한할 필요도 없다.  위의 예시는 함께 제한한 예시
    이다. 
    위의 YAML 파일에서는 네임스페이스를 default로 지정했기에 default 네임스페이스에서 사용할 수 있는 총 자원의 할당량을 제한할 것이다. 단일 포드의 자원
    할당량을 제한하는 것이 아닌, 네임스페이스에서 사용할 수 있는 자원 할당량의 합에 대한 제한이라는 점에 유의하자. ResourceQuota를 생성한 뒤 리소스 정보를
    출력하면
    
            'kubectl apply -f resource-quota.yaml'
            'kubectl describe quota' 
            
    ResourceQuota의 정보에 현재 default 네임스페이스에 생성된 포드들의 자원 할당량 합이 출력된다. 새롭게 생성되는 포드가 한계치보다 더 많은 자원을
    사용하려고 하면 포드를 생성하는 API 요청은 실패한다. 예를 들어, 위 예시에서는 500m의 limits.cpu를 가지는 3개의 포드는 생성할 수 있지만(1500m), 
    그 뒤로는 limits.cpu를 가지는 포드를 더 이상 생성할 수 없다. 단, ResourceQuota를 생성하기 이전에 존재하고 있던 포드들이 이미 자원을 한계치보다
    많이 사용하고 있다고 해서 기존 포드가 종료되지는 않는다. 
    예시로 메모리를 과도하게 사용하는 포드를 생성해보자
        'kubectl run memory-over-pod --image=nginx --generator=run-pod/v --requests="cpu=200m,memory=300Mi" --limits="\
        cpu=200m,memory=3000Mi"'
    방금 생성한 ResourceQuota는 default 네임스페이스에서 최대 1000Mi와 메모리만 사용할 수 있도록 설정했지만, 위 명령어로 3000Mi의 메모리를 요구
    했기 때문에 생성에 실패했다. 그렇다면 디플로이먼트를 통해 생성하면 어떻게 될까?
```
```yaml
apiVersion: apps/v1
kind: Deployment
...
      resources:
        limits: 
          memory: '3000Mi'
          cpu: '1000m'
        requests:
          memory: '128Mi'
          cpu: '500m'
```
```dockerfile
    'kubectl apply -f deployment-over-memory.yaml'
    
    이상하게도 디플로이먼트는 정상적으로 생성된다. 하지만 포드 목록을 출력하면 'kubectl get pods | grep deployment-over-memory' 출력 결과가
    없다는 것을 확인할 수 있다. 여기서 짚고 넘어가야할 것은 '포드를 생성하는 주체는 디플로이먼트가 아닌 레플리카셋'이라는 것이다. 디플로이먼트는
    포드를 생성하기 위한 레플리카셋의 메타데이터를 선언적으로 가지고 있을 뿐, 디플로이먼트 리소스가 직접 포드를 생성하지 않는다. 따라서 포드 생성 거부에 대한
    에러가 레플리카셋에 남아있지 않을 것이다. 
    
        'kubectl get replicasets'
        'kubectl describe rs [레플리카셋 이름]' -> 에러 로그 출력
        
    레플리카셋은 지속적으로 라벨 셀렉터에 해당하는 포드를 생성하려 시도하기 때문에 ResourceQuota가 업데이트되거나 가용 자원이 발생하면 포드를 정상적으로
    생성할 것이다. 
    
    
    - ResourceQuota로 리소스 개수 제한하기 
    ResourceQuota는 자원의 사용뿐만 아니라 디플로이먼트, 포드, 시크릿 등의 리소스 개수를 제한할 수도 있다. ResourceQuota는 아래의 쿠버네티스 오브젝트
    개수를 제한할 수 있다.
    
        1. 디플로이먼트, 포드, 서비스, 시크릿, 컨피그맵, 퍼시스턴트 볼륨 클레임 개수
        2. NodePort 타입의 서비스 개수, LoadBalancer 타입의 서비스 개수
        3. QoS 클래스 중에서 BestEffort 클래스에 속하는 포드의 개수 
        
    포드나 서비스의 최대 개수를 제한하려면 YAML 파일에 count/pod와 같은 형식으로 정의한다. 이전에 사용하던 ResourceQuota YAML을 수정해보자.
```
quota-limit-pod-svc.yaml
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-quota-example
  namespace: default
spec:
  hard:
    requests.cpu: "1000m"
    requests.memory: "500Mi"
    limits.cpu: '1500m'
    limits.memory: '1000Mi'
    count/pods: 3
    count/services: 5
```
```dockerfile
    위의 YAML 파일을 적용하려면 default 네임스페이스에서는 최대 포드 3개, 서비스 5개를 생성할 수 있다. 그 이상 리소스를 생성하려고 하면 API요청이 거절
    된다. 예를 들어, ResourceQuota에서 제한된 개수 이상으로 생성 시도하면 [exceeded quota: ~~]와 같이 에러메시지를 출력한다.
    제한 가능한 다른 쿠버네티스 오브젝트의 경우, :리소스 개수 -> count/<오브젝트 명>.<API 그룹 이름> 
    추가적으로 오브젝트가 어떤 API 그룹에 속하는지 'kubectl api-resources' 명령어로 확인할 수 있다.
        ex)hard:
            count/resourcequotas: 3
            count/secrets: 5
            count/configmaps: :5
            count/persistentvolumeclaims: 2
            count/services.nodeports: 3
            count/services.loadbalancers: 1
            count/deployments.apps: 0


    - ResourceQuota로 BestEffort 클래스의 포드 개수 제한하기 
    ResourceQuota를 사용하면 Request와 Limits가 모두 설정돼 있지 않아서 노드의 자원을 제한할 수 없는 포드, 즉 BestEffort 클래스의 포드 개수를 
    제한할 수도 있다. 
```
quota-limit-baseeffort.yaml
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-quota
  namespace: default
spec:
  hard:
    count/pods: 1
  scopes:
    - BestEffort
```
```dockerfile
    이번에는 이전과 달리 scopes라는 항목을 새롭게 정의했다. scopes는 필수 항목은 아니지만, BestEffort 및 Terminationg, NotTerminateing,
    NotBestEffort와 같은 포드의 상태를 값으로 입력할 수 있다. 위 예시에서는 BestEffort 클래스의 포드 개수를 제한하기 위해서 scopes에 BestEffort
    를 설정했다.
    BestEffort 클래스의 포드는 아무런 자원 할당을 설정하지 않은 경우에 해당하기 때문에 hard 항목에서 limit.cpu나 limit.memory와 같은 자원 제한
    설정과 연결되어 사용하지 않는다. BestEffort 클래스의 포드 개수를 제한할 때는 위의 YAML처럼 포드 개수를 제한하는 항목 (count/pods)만 유효하게
    작동한다. 
        {
            scope에서 NotBestEffort는 BestEffort 클래스가 아닌 QoS클래스를 의미하며, Terminating은 포드의 종료 시간(activeDeadline)이 
            명시적으로 설정된 경우를 의미한다. 이는 보통 잡(Job)이라고 하는 쿠버네티스 오브젝트에서 사용되기 때문에 생성하는 대부분의 포드는 NotTermi
            nating에 속한다고 생각하면 된다. NotBestEffort, Terminating, NotTerminating을 scopes에 설정하는 경우에는 limit.cpu나 
            limit.memory와 같은 자원 제한 및 count/pods와 함꼐 연결해서 사용할 수 있다.
        }
    
    아무런 포드가 생성돼 있지 않다고 가정하고, 위의 ResourceQuota를 생성한 다음 BestEffort 클래스의 포드를 여러 개 생성해보자. 제한을 넘어 포드를 생
    성하려고 시도하면 이 또한 요청이 거절될 것이다.
    
        #생성된 포드, ResourceQuota 삭제
        'kubectl delete quota --all && kubectl delete pod --all && kubectl delete deploy --all'
        'kubectl apply -f quota-limit-besteffort.yaml'
        'kubectl run besteffort-1 --image=nginx --genenrator=run-pod/v1'
        'kubectl run besteffort-2 --image=nginx --gnenrator=run-pod/v1'
        
    하지만 이렇게 명시적으로 BestEffort 포드의 개수를 허용하지 않은 채로 ResourceQuota에서 메모리나 CPU를 제한한다면 BestEffort 포드의 생성은
    실패한다. CPU와 메모리를 제한하는 ResourceQuota만 생성해 둔 다음, 어떠한 자원도 할당하지 않은 BestEffort 클래스의 포드를 생성해보자.
    
        #생성된 포드, ResourceQuota 삭제
        'kubectl delete quota -all && kubectl delete pod -all'
        #이전에 사용했던 CPU, 메모리를 제한하는 ResourceQuota 재생성
        'kubectl apply -f resource-quota.yaml'
        
        'kubectl run besteffort-1 --image=nginx --generator=run-pod/v1'
         -> failed:quota: resource-quota example: must specify limits.cpu, limits.memory, requests.cpu, requests.memory
     ResourceQuota에 limits.cpu나 limits.memory 등을 이용해 네임스페이스에서 사용가능한 자원의 합을 설정했다면 포드를 생성할 때 반드시
     해당 항목을 함께 정의해야한다.그렇지 않으면 위와 같은 에러가 반환된다.
     
     이를 위해서 쿠버네티스에서는 포드의 자원 사용량을 기본적으로 제한할 수 있는 LimitRange라는 기능을 제공한다.
     
     
            > 1.5.2 LimitRange로 자원 사용량 제한
    LimitRange는 특정 네임스페이스에서 할당되는 자원의 범위 또는 기본값을 지정할 수 있는 쿠버네티스 오브젝트이다. LimitRange의 용도를 간단히 보면
    아래와 같다.
        1. 포드의 컨테이너에 CPU나 메모리 할당량이 설정돼 있지 않은 경우, 컨테이너에 자동으로 기본 Requests 또는 Limits 값을 설정할 수 있다.
        2. 포드 또는 컨테이너의 CPU, 메모리, 퍼시스턴트 볼륨 클레임 스토리지 크기의 최소/최대값을 지정할 수 있다.
    LimitRange도 ResourceQuota와 마찬가지로 네임스페이스별로 적용할 수 있는 기능이므로 네임스페이스에 종속되는 오브젝트이다. 기본적으로 어떠한 Limit
    Range도 생성돼 있지 않다. 
    
        'kubectl get limitranges'
        'kubectl get limits' #limits라는 이름으로도 사용 가능
    
    LimitRange를 아래의 YAML을 보면서 이해해보자
```
limitrange-example.yaml
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits: 
  - default:          # 1. 자동으로 설정될 기본 Limits 값
      memory: 256Mi
      cpu: 200m
    defaultRequest:   # 2. 자동으로 설정될 기본 Requests 값
      memory: 128Mi
      cpu: 100m
    max:
      memory: 1Gi     # 3. 자원 할당량의 최댓값
      cpu: 1000m
    min:              # 4. 자원 할당량의 최솟값
      memory: 16Mi
      cpu: 50m
    type: Container   # 5. 각 컨테이너에 대해서 적용
```
```dockerfile
    1. default: 포드의 컨테이너에 Limits 값이 설정돼 있지 않다면 자동으로 이 값을 Limits로 설정한다.
    2. defaultRequest: 포드의 컨테이너에 Requests 값이 설정돼 있지 않다면 자동으로 이 값을 Requests로 설정한다.
    3. max: 포드의 컨테이너에 설정될 수 있는 Limits 값의 최대치를 의미한다. 만약 max에 설정된 값보다 더 많은 자원을 사용하려고 하면 포드의 생성은 실패한다.
    4. min: 포드의 컨테이너에 설정될 수 있는 Requests 값의 최소치를 의미한다. 만약 min에 설정된 값보다 자원을 더 적게 사용하려고 시도하면 포드의
    생성은 실패한다.
    5. 이러한 자원 할당에 대한 설정이 컨테이너 단위로 적용될 것임을 의미한다. 컨테이너 외에도 Pod, PersistentVolumeClaim을 입력할 수 있다. 
    
    이 YAML 파일로 LimitRange를 생성한 뒤 BestEffort 클래스의 포드를 생성해 보면 자동으로 Requests와 Limits 값이 설정되는 것을 확인할 수 있다.
        'kubectl apply -f limitrange-example.yaml'
        'kubectl run pod-limitrange-example --image-nginx --generator=run-pod/v1'
        'kubectl describe pod pod-limitrange-example'
    마찬가지로 min과 max의 범위를 벗어나는 포드의 컨테이너는 생성할 수 없다. 
        'kubectl run test --image=nginx --generator=run-pod/v1 --requests="cpu=10m, memory=16Mi" --limits="cpu=100m, memory=500Mi"'
        'kubectl run test --image=nginx --generator=run-pod/v1 --requests="cpu=50m, memory=16Mi" --limits="cpu=1001m, memory=500Mi"'
    LimitRagne에서 maxLimitRequestRatio 항목을 사용하면 포드의 컨테이너에서 오버커밋되는 자원에 대한 비율을 제한할 수도 있다. 예를 들어 아래와
    같이 생성했다고 가정해 보자.
```
limitrange-ratio.yaml
```yaml
apiVersion: v1
kind: LimitRange
metadata: 
  name: limitrange-ratio
spec:
  limits:
  - maxLimitRequestRatio:
      memory: 1.5
      cpu: 1
    type: Container
```
```dockerfile
    위 예시에서는 maxLimitRequestRatio.memory의 값을 1.5로 설정했으며, 이는 새롭게 생성되는 포드의 Limits, Requests의 비율은 1.5보다 반드시
    작아야만 한다는 의미한다.
    간단한 예시로 메모리의 Limits가 200Mi이고 Requests가 100Mi로 설정된 포드의 컨테이너를 생성하려고 시도한다고 생각해 보자. 이 포드의 LimiRequestRatio
    값 200Mi / 100Mi = 2이지만, LimitRange에는 Limits와 Request의 비율을 최대 1.5까지만 허용하고 있기 때문에 이 포드의 생성은 거절될 것이다.
    
        'kubectl apply -f limirage-ratio.yaml'
        'kubectl run test --image=nginx --generator=run-pod/v1 --requests="cpu=100m, memory=100Mi" \
        --limits="cpu=100m, memory=200Mi"'
        
    maxLimitRequestRatio는 오버커밋을 얼마나 허용할 수 있는지 제어할 수 있을 뿐만 아니라, 이 값을 1로 설정함으로써 반드시 Guaranteed 클래스의
    포드만을 생성하도록 강제하는 용도로 사용할 수도 있다. 
    
    만약 포드 단위로 자원 사용량의 범위를 제한하고 싶다면 아래의 YAML 파일처럼 정의해 사용할 수 있다. 이 때의 포드의 사용량은 포드에 존재하는 모든 컨테이너의
    자원의 합이 된다. 예를 들어, 아래의 LimitRange는 포드의 컨테이너들에 할당된 메모리의 합이 최소 200Mi이여야 하며, 최대 1Gi까지 허용된다는 것을
    의미한다. 
```
limitrange-example-pod.yaml
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limit-range
spec:
  limits:
  - max:
      memory: 1Gi
    min:
      memory: 200Mi
    type: Pod
```
```dockerfile
    ResourceQuota에서 네임스페이스의 Limits, Requests를 설정하면 기본적으로 BestEffort 클래스의 포드 생성이 거부되지만, LimitRange를 사용하면
    포드의 컨테이너에 일괄적으로 기본 자원 할당량을 설정할 수 있다는 점을 숙지하자.
    
    
    
                > 1.6 ResourceQuota, LimitRange의 원리 : Admission Controller
    이전에 service account를 설명할 때 어드미션 컨트롤러(Admission Controller)에 대해서 알아본 적이 있다. 사용자가 'kubectl' 등을 통해
    쿠버네티스의 API 서버에 요청을 보낼 때, 인증과 인가 외에도 어드미션 컨트롤러라는 단계가 존재했다.
    
    어드미션 컨트롤러에 대해 간단하게 설명하면 '사용자의 API 요청이 적절한지 검증하고, 필요에 따라 API 요청을 변형하는 단계'라고 할 수 있다. 만약 kubectl
    등으로부터 전송된 API가 부적절하다면 어드미션 컨트롤러 단계에서 해당 API 요청을 거절할 수도 있고, 필요하다면 API 요청에 포함된 파라미터를 변경할 수도 
    있다. 
    
    지금까지 여러분 어드미션 컨트롤러에 대해 직접적으로 신경 쓴 적은 없었지만, 이미 여러 가지 어드미션 컨트롤러를 사용하고 있었다. 대표적으로 앞서 사용해
    봤던 service account가 바로 어드미션 컨트롤러의 한 종류였다. 그뿐만 아니라. 방금 사용했던 ResourceQuota와 LimitRange 또한 어드미션 컨트롤러
    의 한 종류이다.
    
    이와 같은 동작 방식을 구현하기 위해서 쿠버네티스에는 총 두 단계의 어드미션 컨트롤러가 있다. API 요청을 검사하는 것을 검증(Validating) 단계라고 부르며,
    API 요청을 적절히 수정하는 것을 변형(Mutating) 단계라고 한다. 이 두 가지 단계가 있다는 사실에 유의하면서 포드를 생성하는 API 요청이 ResourceQuota
    와 LimitRange 어드미션 컨트롤러에 의해 어떻게 조작되는지 생각해보자. 
    
        1. 사용자가 kubectl apply -f pod.yaml와 같은 명령어로 API 서버에 요청을 전송했다.
        2. x509 인증서, service account 등을 통해 인증 단계를 거친다.
        3. 롤, 클러스터 롤 등을 통해 인가 단계를 거친다.
        4. 어드미션 컨트롤러인 ResourceQuota는 해당 포드의 자원 할당 요청이 적절지 검증(Validating)한다. 만약 해당 포드로 인해 ResourceQuota
        에 설정된 네임스페이스의 최대 자원 할당량을 초과한다면 해당 API 요청은 거절된다. 
        5. 해당 API 요청에 포함된 포드 데이터에 자원 할당이 설정되지 않은 경우, 어드미션 컨트롤러인 LimitRange는 포드 데이터에 CPU 및 메모리 할당의
        기본값을 추가함으로써 원래의 포드 생성 API의 데이터를 변형한다. 
        
    위 단계 중 4,5번이 어드미션 컨트롤러가 동작하는 단계이다. 위 예시에서는 ResourceQuota와 LimitRange를 통해 단적인 예시만 알아봤지만 실제로
    'kubectl' 명령어를 사용할 때 실제로 동작하는 어드미션 컨트롤러는 더 많을 수 있다. 
    
    이러한 기본적인 어드미션 컨트롤러는 대부분 기본적으로 활성화돼 있기 때문에 별도로 신경 쓸 필요는 없지만, 필요하다면 커스터마이징한 어드미션 컨트롤러를 
    직접 구현해 쿠버네티스에 등록하는 것 또한 가능하다. 예를 들어 Nginx 포드를 생성하는 API 요청이 제출됐지만, 개발자의 실수로 Nginx 컨테이너가 80번
    포트가 아닌 다른 포트를 사용하도록 YAML 파일에 정의돼 있다면 이를 자동으로 수정해주는 어드미션 컨트롤러를 직접 구현할 수 있다.
    
    그뿐만 아니라 포드의 어노테이션에 따라서 별도의 사이드카 컨테이너를 서비스 메쉬(Service Mesh) 솔류션인 Istio 또한 어드미션 컨트롤러를 통해 포드에
    프록시 사이드카 컨테이너를 주입하는 방법을 사용한다. 
    
   
            > 2. 쿠버네티스 스케쥴링
    클라우드에서 자원 제한 기능과 함께 고려해야할 중요한 기능은 스케쥴링이다. 여기서 말하는 스케쥴링이란 컨테이너나 가상 머신과 같은 인스턴스를 새롭게 생성할
    때, 그 인스턴스를 어느 서버에 생성할 것인지 결정하는 일을 의미한다.
    쿠버네티스와 같은 클라우드 시스템에서 스케쥴링이 중요한 이뉴는 상황에 따라서 매우 다양하다. 간단한 예로 특정 컨테이너가 빠른 파일 입출력을 위해서 SSD를
    사용해야 한다면 SSD를 가지고 있는 서버에 할당할 수도 있다. 또는 컨테이너를 모든 서버에 최대한 고르게 배포함으로써 서버에 장애가 발생해도 애플리케이션의
    무중단, 즉 고가용성(HA: High Availablity)을 확보해야 할 수도 있다. 이처럼 컨테이너를 생성하기 전에 특정 목적에 최대한 부합하는 워커 노드를 선택하는
    작업이 스케쥴링에 해당한다.
    
    쿠버네티스에서는 포드를 생성할 워커 노드를 선택할 수 있도록 다양한 스케쥴링 방법을 제공하고 있다. 위와 같이 간단한 예시를 위한 스케쥴링 전략 외에도, 
    조금 더 복잡한 스케쥴링 전략을 직접 선택해서 구현할 수도 있다. 이와 이어서 포드를 할당할 워커 노드를 결정하는 스케쥴링 과정을 알아보고 포드를 생성하기
    위한 YAML 파일에 사용할 수 있는 여러 스케쥴링 설정 값을 알아보자.
    
    
            > 2.1 포드가 실제로 노드에 생성되기까지의 과정
    포드를 스케쥴링할 수 있는 옵션들을 알아보기 전에, 먼저 쿠버네티스에서 스케쥴링이 어떻게 수행되는지 알아보자. 사용자가 kubectl이나 API 서버로 포드 생성
    요청을 전송하면 어떠한 일이 일어나는지 정리해보자.
    
        1. ServiceAccount, RoleBinding 등의 기능을 이용해 포드 생성을 요청한 사용자의 인증 및 인가 작업을 수행한다.
        2. ResourceQuota, LimitRange와 같은 어드미션 컨트롤러가 해당 포드의 요청을 적절히 변형(Mutating)하거나 검증(Validating)한다.
        3. 어드미션 컨트롤러의 검증을 통과해 최종적으로 포드 생성이 승인됐다면 쿠버네티스는 해당 포드를 워커 노드 중 한 곳에 생성한다.
    
    포드의 스케쥴링은 위 단계 중에서 3번에서 수행된다. 이전 단계들은 모두 성공적으로 통과했다고 가정하고, 3번을 더 자세히 보자. 
    쿠버네티스에는 여러 가지 핵심 컴포넌트들이 기본적으로 실행되고 있으며, 이 컴포넌트들은 kube-system 네임스페이스에서 실행되고 있다. kube-system 
    네임스페이스에는 kubectl로 상호 통신할 수 있는 API 서버(kube-apiserver) 컴포넌트 외에도 다양한 핵심 컴포넌트가 있다. 그중에서 스케쥴링에 관하여는
    컴포넌트 kube-scheduler와 etcd이다. kube-scheduler는 쿠버네티스 스케쥴러에 해당하며, etcd는 쿠버네티스 클러스터의 전반적인 상태 데이터를 저장하는
    일종의 DB 역할을 한다. 
    
    kube-scheduler와 etcd 또한 포드로써 실행되기 때문에 kubectl get pods 명령어로 쉽게 확인할 수 있다. 
        
            'kubectl get pods -n kube-system'
        
    etcd는 분산 코디네이터(Distributed Coordinator)라는 불리는 도구의 일종으로, 클라우드 플랫폼 등의 환경에서 여러 컴포넌트가 정상적으로  상호 작용
    할 수 있도록 데이터를 조정하는 역할을 담당한다. 쿠버네티스 또한 클러스터 운용에 필요한 정보를 분산 코디네이터인 etcd에 저장하는데, 현재 생성된 
    디플로이먼트나 포드의 목록과 정보, 클러스터 자체의 정보 등과 같은 대부분의 데이터가 etcd에 저장돼 있다.
    etcd에 저장된 데이터는 무조건 API 서버(kube-apiserver)를 통해서만 접근할 수 있다. 예를 들어, 우리들이 kubectl get pods와 같은 명령어를 
    실행하면 API 서버에 요청이 전달되고, API 서버는 etcd의 데이터를 읽어와 kubectl 사용자에게 반환한다고 생각하면 된다.
    
    etcd에 저장된 포드의 데이터에는 해당 포드가 어느 워커 노드에서 실행되는지를 나타내는 nodeName 항목이 존재한다. nodeName 항목은 다음과 같이 
    kubectl get 명령어로도 쉽게 확인할 수 있다. 
    
        'kubectl get pods mypod -o yaml | grep -F3 nodeName'
        
    인증, 인가, 어드미션 컨트롤러 등의 단계를 모두 거쳐 포드 생성 요청이 최종적으로 승인됐다면 API 서버는 etcd에 포드의 데이터를 저장한다. 하지만 API
    서버는 포드의 데이터 중에서 nodeName 항목을 설정하지 않은 상태로 etcd에 저장한다. 아직은 스케쥴링이 수행되지 않았기 때문에 포드가 실제로 생성되지
    않은, 단순히 데이터로만 저장된 상태이기 떄문이다.
    
    쿠버네티스 스케쥴러 컴포넌트에 해당하는 kube-scheduler는 API 서버의 Watch를 통해 nodeName이 비어 있는 포드 데이터가 저장됐다는 사실을 전달
    받는다. 즉, 사용자의 포드 생성 요청에 의해 포드의 데이터가 etcd에 저장되긴 했지만, 아직 특정 노드에 스케쥴링되지 않은 포드를 감지하는 것이다.
    스케쥴러(kube-scheduler)는 nodeNmae이 설정되지 않은 해당 포드를 스케쥴링 대상으로 판단하고, 포드를 할당할 적절한 노드를 선택한 다음 API 서버에게
    해당 노드와 포드를 바인딩할 것을 요청한다. 그리고 나면 포드의 nodeName 항목의 값에는 선택된 노드의 이름이 설정된다.  클러스터의 각 노드에서 실행 중인
    kubelet은 API 서버에 둔 Watch를 통해 포드의 데이터에서 nodeName이 설정됐다는 소식을 전달받는다. 그리고 나서야 해당 nodeName에 해당하는 노드의 
    kubelet이 컨테이너 런타임을 통해 포드를 생성합니다. 여기까지가 쿠버네티스의 포드가 최종적으로 생성되기까지의 과정이다. 
    
        
            > 2.2  포드가 생성될 노드를 선택하는 스케쥴링 과정
    위 과정에서 가장 중요한 과정은 '스케쥴러가 적절한 노드를 어떻게 선택하느냐'이다. 스케쥴러는 크게 노드 필터링, 노드 스코어링 단계를 거쳐 최종적으로 노드를
    선택한다. 
    
        - 노드 필터링 : 포드를 할당할 수 있는 노드와 그렇지 않은 노드를 분리해서 걸러내는(Filtering) 단계이다. 예를 들어 포드에 설정된 CPU나 메모리의
        Requests만큼의 가용 자원이 존재하지 앟는 노드는 노드 필터링 단계에서 제외될 것이다. 그 외에도 기본적으로 마스터 노드는 포드를 할당할 수 없는 노드로
        취급되며, 장애가 발생해 kubectl get nodes에서 STATUS가 Ready가 아닌 워커 노드 또한 제외된다. 노드 필터링 단계에서 선정된 노드의 목록은 노드
        스코어링 단계로 전달된다.
        
        - 노드 스코어링: 노드 스코어링 단계에서는 쿠버네티스의 소스 코드에 미리 정의된 알고리즘의 가중치에 따라서 노드의 점수를 계산한다. 예를 들어, 포드가
        사용하는 도커 이지미가 이미 노드에 존재할 때는 빠르게 포드를 생성할 수 있기 때문에 해당 노드의 점수가 증가한다.(Image Locality). 또는 노드의
        가용 자원이 많으면 많을 수록 점수가 높게 평가될 수도 있다 (Least Requested). 이러한 알고리즘들의 값을 합산함으로써 후보 노드들의 점수를 계산한
        다음, 가장 점수가 높은 노드를 최종적으로 선택한다.  
    
    노드 스코어링은 쿠버네티스에 내장된 로직에 의해 계산되기 때문에 우리가 직접 점수를 매기는 알고리즘을 수정하는 경우는 많지 않다. 대부분은 스케쥴링 조건을
    포드의 YAML로 설정하여 노드 필터링 단계에 적용할 수 있도록 구성하는 것이 일반적이다.
    
    
    
            > 2.3 NodeSelector와 Node Affinity, Pod Affinity
            nodeName과 nodeSelector를 사용한 스케쥴링 방법
    특정 워커 노드에 포드를 할당하는 가장 확실한 방법은 포드의 YAML 파일에 노드의 이름(nodeName)을 직접 명시하는 것이다. 'kubectl get nodes' 명령어
    에서 출력된 노드의 이름을 다음과 같이 nodeName 항목에 명시한 다음 포드를 생성하면 해당 노드에 포드가 할당된다.
        
            'kubectl get nodes'
```
```yaml
apiVersion: v1,
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: [노드 이름]
  containers: 
  - name: nginx
    image: nginx:latest
```
```dockerfile
    포드를 생성한 다음, 해당 포드의 위치를 확인해 보면 nodeName에 설정한 노드에 할당됐음을 알 수 있다. 
    
        'kubectl apply -f nodename-nginx.yaml'
        
    하지만 이러한 방식의 포드 스케쥴링은 바람직하지 않다. 우선 노드의 이름을 고정으로 설정했기에 다른 환경에서 해당 YAML을 보편적으로 활용할 수 없다.
    또한 노드에 장애가 발생했을 때도 유연하게 대처할 수 없다. 
    
    nodeName 대신에 사용할 수 있는 여러 가지 다른 방법이 있지만, 그중에서 가장 쉽게 사용할 수 있는 방법은 노드의 라벨(Label)을 사용하는 것입니다. 라벨을
    이용하면 특정 라벨이 존재하는 노드에만 포드를 할당할 수 있다. 노드의 라벨은 쿠버네티스가 자동으로 설정해 놓은 것도 있지만, 필요에 따라 여러분이 직접 라벨
    을 추가할 수도 있다. 
    
    kubectl get nodes --show-labels 명령어를 실행해 보면 쿠버네티스가 기본적으로 설정했 놓은 노드의 라벨을 확인할 수 있다. 미리 설정된 라벨들은
    대부분 kubernetes.io/라는 접두어로 시작하는데, 이는 쿠버네티스에 의해 미리 예약돼 사용하는 것을 의미한다. 
    
        'kubectl get nodes --show-labels'
    
    지금까지 사용해 온 라벨과 동일하게 라벨은 <키=값> 형태로 설정된다. 기본적으로 설정되는 라벨에는 해당 노드의 OS(Linux), CPU 아키텍쳐(amd64),
    호스트 이름 등이 있다. 이 라벨을 사용해도 큰문제는 없지만, 이번에는 3개의 워커 노드 중에서 1개의 노드는 'mylabel/disk'='ssd' 라벨을, 2개의
    노드는 'mylabel/disk'='hdd' 라벨을 추가해보자.
    
    노드에 라벨을 추가하려면 kubectl label nodes < 노드 이름 > < 추가할 라벨 > 과 같이 명령어를 사용하면 된다. 
        'kubectl label nodes [node이름] mylabel/disk=ssd'
    나머지 두 개의 노드에 대해서도 라벨을 추가하되, mylabel/disk=hdd라는 라벨을 설정한다.
    
        {
            노드에 설정된 라벨을 삭제하려면 라벨 키의 이름에 -(대시)를 추가하면 된다. 예를 들어 mylabel/disk를 삭제하려면
            
            'kubectl label nodes [노드 이름] mylabel/disk-'
        }
    
    이때 mylabel/disk 키의 값이 hdd인 노드에 포드를 할당하려면 포드의 YAML 파일에 아래와 같이 nodeSelector를 정의한다.
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeselector
spec:
  nodeSelector:
    mylabel/disk: hdd
  containers:
  - name: nginx
    image: nginx:latest
```
```dockerfile
    mylabel/disk: hdd라는 라벨을 갖는 노드가 2개 이상이라면 해당 노드 중 하나가 선택된다. 즉, nodeSeelctor는 해당 라벨이 존재하는 노드 중 하나를
    선택하기 때문에 적어도 노드의 이름에 종속적이지 않게 YAML 파일을 작성할 수 있다.
    단, 이러한 포드 스케쥴링 방법은 각 단일 포드에 대해 수행한다는 점을 헷깔리면 안된다. 예를 들어 내부적으로 포드를 생성하는 디플로이먼트나 레플리카셋을
    생각해보자. 여러 개의 포드를 생성하도록 설정된 디플로이먼트의 포드들은 한꺼번에 동일하게 스케쥴링되는 것이 아니라 각 포드가 하나씩 독립적으로 스케쥴링
    한다. 즉, 아래의 YAML과 같이 디플로이먼트에 라벨을 통해서 nodeSelector 항목을 정의했더라도 디플로이먼트의 모든 포드가 동일한 하나의 노드에 할당
    되는 것은 아니라는 것이다. 
```
deployment-nginx-node-selector.yaml
```yaml
...
  replicas: 3
  selector:
    matchLabels:
      app: deployment-nginx
  template:
    metadata:
      name: deployment-nginx
      labels:
        app: deployment-nginx
    spec:
      nodeSelector:
        mylabel/disk: hdd
```
```dockerfile
        > Node Affinity를 이용한 스케쥴링 방법
    nodeSelector도 나쁘지 않은 방법이지만, 단순히 라벨의 키-값이 같은지만 비교해서 노드를 선택하기 떄문에 활용 방법이 다양하지 않다. 이를 보완하기 위해
    쿠버네티스는 Node Affinity라는 스케쥴링 방법을 제공한다. Node Affinity는 nodeSelector에서 조금 더 확장된 라벨 선택 기능을 제공하며,
    반드시 충족해야하는 조건(Hard)과 선호하는 조건(Soft)을 별도로 정의할 수도 있다.  Node Affinity에는 2가지 종류의 옵션이 있다.
    
        - requiredDuringSchedulingIgnoredDuringExecution
        - preferredDuringSchedulingIgnoredDuringExecution
        
    위 두 옵션은 다른 기능을 수행한다. 먼저 requiredDuringSchedulingIgnoredDuringExecution를 사용하며 예시를 보자
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeaffinity-required
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: mylabel/disk
            operator: In
            values:             # values의 값 중 하나만 만족해도 된다.
              - ssd
              - hdd 
  containers:
  - name: nginx
    image: nginx:latest
```
```dockerfile
    굉장히 양이 많아 보이지만, '- matchExpression'과 하위 항목들을 보면 된다. 위 YAML 파일에서 "operator: In" 항목은 key의 라벨이 values의
    값 중 하나를 만족하는 노드에 포드를 스케쥴링한다는 것이다. 따라서 노드에 설정된 라벨이 mylabel/disk=ssd 또는 mylabel/disk=hdd라면 해당
    노드에 포드가 할당될 것이다. 
    
    이처럼 Node Avfinity는 여러 개의 키-값을 정의한 뒤 operator를 통해 별도의 연산자를 사용할 수 있다. 이때 operator에는 In 외에도 NotIn,
    Exsits, DoesNotExist, Gt(grater than), Lt(less than)을 사용할 수 있어서 nodeSelector보다 더욱 다양하게 활용할 수 있다.
    
    단, 위의 YAML 파일에서는 requiredDuringSchedulingIgnoredDuringExecution 옵션을 사용했다는 것을 유의해야한다. required...라는 이름에서
    볼 수 있듯이 '반드시 만족해야만 하는 제약 조건'을 정의할 때 쓰인다. 따라서 nodeSelector의 기능을 확장했다고 생각하면 쉽다.
    
    하지만 preferredDuringSchedulingIgnoredDuringExecution 옵션은 preferred라는 이름이 나타내는 것처럼 '선호하는 제약 조건'을 의미한다.
    따라서 preferredDuringSchedulingIgnoredDuringExecution 아래에 정의한 키-값 조건은 반드시 만족할 필요는 없으며, 만약 해당 조건을 만족하는
    노드가 있으면 그 노드를 조금 더 선호하겠다는 의미가 된다. 
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodeaffinity-preferred
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80      #가중치 (1~100)
        preference:
          matchExpressions:
          - key: mylabel/disk
            operator: In
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx:latest
```
```dockerfile
    이번에는 required.. 대신 preferred를 사용했다. weight라는 값이 눈에 보이는데, 이 값은 조건에 해당하는 노드를 얼마나 선호할지를 나타내는 가중치
    이다. 즉, 때에 따라서는 matchExpressions의 조건을 만족하는 노드를 반드시 선택하지 않을 수도 있지만, 가급적이면 해당 노드를 선택하도록 가중치를 부여
    한다고 보면 된다. 이 가중치는 할당 가능한 모든 노드를 필터링한 뒤 수행하는 노드 스코어링 단계에서 적용된다.
    
    위의 YAML을 생성하면  mylabel/disk=ssd 라벨을 갖고 있지 않은 노드에도 포드가 할당될 수 있지만, mylabel/disk=ssd 라벨을 설정한 노드에 할당될
    확률이 높아지기 때문에 일반적으로 이를 soft 제약조건이라고 한다. 
    
        'kubectl apply -f nodeaffinity-preferred.yaml'
        'kubectl get pods -o wide'
    단, 이러한 스케쥴링 조건은 포드를 할당할 당시에만 유효하다. 따라서 일단 포드가 할당 뒤에 노드의 라벨이 변경되더라도 다른 노드로 포드가 옮겨가는 퇴거(Eviction)
    이 발생하지는 않는다. requiredDuringSchedulingIgnoredDuringExecution라는 이름에서 이 동작의 원리를 알 수 있다. '스케쥴링 과정에서는
    유효하지만(required during scheduling), 일단 시작한 다음에는 무시된다.(ignored during execution)'
    
        {
            반대되는 옵션으로  requiredDuringSchedulingRequiredExecution처럼 접미어를 바꾸어 사용할 수도 있다. 이 경우 스케쥴링에 영향을 주는 노드
            라벨이 포드가 실행된 뒤에 변경됐다면 포드가 다른 노드로 옮겨 간다. 
        }
```



````dockerfile
        > Pod Affinity를 이용한 스케쥴링 방법
    Node Affinity가 특정 조건을 만족하는 노드를 선택하는 방법이라면, Pod Affinity는 특정 조건을 만족하는 포드와 함께 실행되도록 스케쥴링한다. 
    사용 방법은 Node Affinity와 거의 같기 때문에 이전에 사용했던 requiredDuringSchedulingIgnoredDuringExecution과 같은 옵션을 똑같이 사용할
    수 있다. 
````
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-podaffinity
spec: 
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: mylabel/database
            operator: In
            values:
            - mysql
        topologyKey: failure-domain, beta, kubernetes.io/zone
  containers:
  - name: nginx
    image: nginx:latest
```
```dockerfile
    대부분의 항목은 Node Affinity를 사용했을 때와 같지만, nodeAffinity 대신 podAffinity 항목을 사용했다는 점이 다르다. 이번에는 또한 topologyKey
    라는 새로운 항목을 정의했다. 
    
    위의 YAML 파일은 "mylabel/database=mysql"이라는 라벨을 가지는 포드와 함께 위치하도록 스키쥴링하라는 뜻이다. 이는 requiredDuringScheduling
    IgnoredDuringExecution 항목을 통해 이전과 비슷한 의미로 (required..) 사용되고 있지만, 이 라벨을 가지는 포드와 무조건 같은 노드에 할당하라는
    뜻이 아니다. topologyKey는 해당 라벨을 가지는 토폴로지 범위의 노드를 선택한다는 것을 의미한다. 예를 들어 쿠버네티스 노드들이 topologyKey에 할당된
    라벨의 키-값에 따라 여러 개의 그룹(topology)로 분류된다고 생각해보자 이때 matchExpression의 라벨 조건을 만족하는 포드가 위치한 그룹의 노드 중
    하나에 포드를 할당한다. 따라서 조건을 만족하는 포드와 동일한 노드에 할당될 수도 있지만, 해당 노드와 동일한 그룹(topology)에 속하는 다른 노드에 포드가
    할당될 수도 있다. 
    이러한 Pod Affinity 스케쥴링 전략은 다양한 용도로 활용할 수 있지만, 대표적인 활용 예로는 응답 시간을 최대한 줄여야 하는 두 개의 포드를 동일한 가용
    영역(AZ: Available Zone) 또는 리전(Region)의 노드에 할당하는 경우를 생각해 볼 수 있다.
```
```yaml
... 
  - matchExpressions:
    - key: mylabel/database
      operator: In
      values:
      - mysql
    topologyKey: kubernetes.io/hostname
``` 
```dockerfile
        {
            kubernetes.io/hostname 라벨은 쿠버네티스를 설치하면 기본적으로 모든 노드에 설정되는 라벨이다. 이 라벨의 값은 각 노드의 호스트 이름으로
            설정된다. 
        }
```