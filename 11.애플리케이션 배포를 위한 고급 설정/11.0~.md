```dockerfile
                > 애플리케이션 배포를 위한 고급 설정 
            1. 포드의 자원 사용량 제한
    쿠버네티스와 같은 컨테이너 오케스트레이션 툴의 가장 큰 장점 중 하나는 여러 대의 서버를 묶어서 리소스 풀로 사용할 수 있다는 것이다. 클러스터의 CPU나
    메모리 등의 자원이 부족할 떄 필요한 용량만큼 서버를 동적으로 추가함으로써 수평적으로 확장할 수 있기 떄문이다. 하지만 서버를 수평적으로 늘리는 스케일 아웃
    (scale-out) 만큼 중요한 작업이 하나 이다. 바로 클러스터 내부에서 컴퓨팅 자원 활용률(Utilization)을 늘리는 것이다.
    
    자원 활용률은 서버 클러스터에서 자원을 얼마나 효율적으로 빠짐없이 사용하고 있는지를 의미한다. 예를 들어 쿠버네티스에서 실행 중인 컨테이너의 CPU, 메모리
    사용량이 현저히 낮거나 유휴 상태의 컨테이너에게 과잉 할당을 했다면 이를 자원 활용률이 낮다고 표현한다. 이러한 상황을 방지하기 위해서 각 컨테이너에
    적절한 자원 사용량을 제한해야하며, 남는 자원을 사용할 수 있는 전략을 세워야 한다. 
    
    쿠버네티스는 컴퓨팅 자원을 컨테이너에 할당하기 위한 여러 기능을 제공한다. 이번 장에서는 포드나 컨테이너에 CPU, 메모리 등의 자원을 할당하는 기본 방법을
    먼저 알아본 후, 쿠버네티스의 자원 활용률을 높이기 위한 오버커밋(Overcommit) 방법을 알아본다. 그 후 ResourceQuota, LimitRange라는 쿠버네티스 
    오브젝트를 알아본다.
    
            > 1.1 컨테이너와 포드의 자원 사용량 제한 : Limits
    쿠버네티스에서 자원을 할당하는 방법을 알아보기 전 도커 컨테이너의 자원 제한을 다시 상기시켜보자. 컨테이너의 자원 사용량을 제한하는 방법은 여러 가지가 있지만
    '--memory', '--cpu', '--cpu-shares', '--cpu-quota' 및 '--cpu-runtime' 등이 있다. '--memory' 옵션은 컨테이너가 사용 가능한 최대
    메모리 사용량을 제한하며, 그외의 옵션들의 CPU의 사용량을 제한한다. 이 중에서 '--cpu-shares'는 정량적인 CPU 할당량이 아닌 비율 값을 사용한다는
    점에서 특별한 옵션이었다. 
    
        'docker run -it --name memory_1gb --memory 1g ubuntu:16.04'
        'docker run -it --name cpu_1_alloc --cpus 1 ubuntu:16.04'
        'docker run -it --name cpu_shares_example --cpu-shares 1024 ubuntu:16.04'
    
    또는 다음관 같이 자원 제한 옵션을 설정하기 않고 도커 컨테이너를 생성하면 호스트의 모든 CPU, 메모리를 사용할 수 있었다.
    
        'docker run -it --name unlimited_blade unbuntu:16.04'
    
    쿠버네티스는 기본적으로 도커를 컨테이너 런타임으로 사용하기 때문에 포드를 생성할 때 docker와 동일한 원리로 CPu, 메모리 최대 사용량을 제한할 수 있다.
    그렇지만 지금까지 포드나 디플로이먼트 등을 생성할 때 자원 할당량을 명시적으로 제한했던 적이 없다는 것을 눈치챘을 것이다. 이처럼 자원 할당량을 설정하지 
    않으면 포드의 컨테이너가 노드의 물리 자원을 모두 사용할 수 있기 때문에 노드의 자원이 모두 고갈되는 상황이 발생할 수 있다. 
    
    이 예방할 수 있는 가장 간단한 방법은 포드 자체에 자원 사용량을 명시적으로 설정하는 것이다. 포드의 CPU, 메모리 사용량을 제한하기 위해 아래 내용으로 
    YAML을 작성해보자.
```
resource-limit-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-limit-pod
  labels:
    name: resource-limit-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
  resources:
    limits:
      memory: '256Mi'
      cpu: '1000m'
```
```dockerfile
    포드를 정의하는 스펙에 새롭게 spec.containers.resources.limits 항목을 정의했다. 메모리는 256를 입력했는데, 이 설정값은 도커 명령어에서
    'docker run --memory 256m'과 같다. 즉 이 포드의 컨테이너 최대 메모리 사용량은 256Mi으로 제한된다. 
    cpu에는 1개의 cpu를 뜻하는 1000m(밀리코어)라는 값을 입력했으며, 이는 도커 명령에서 'docker run --cpus 1'와 같다. 따라서 이 포드의 컨테이너는
    최대 1개 CPU 만큼의 사이클을 사용할 수 있다. 
    
        {
            이전에 포드에는 여러 개의 컨테이너가 존재할 수 있다. 따라서 하나의 포드에 여러 개의 컨테이너가 할당돼 있다면 각 컨테이너에 대해
            자원을 각각 할당할 수 있다. 
        }
        
    위 내용으로 YAML을 작성한 후 포드를 생성하면 'kubectl apply -f resource-limit-pod.yaml'
    
    'docker info' 명령어로 도커 호스트의 가용 자원을 확인할 수 있던 것처럼 쿠버네티스에서도 'kubectl describe node' 명령어로 워커 노드의 가용 자원을
    간단하게 확인할 수 있다. 이를 위해 방금 생성한 포드가 어느 노드에 생성돼 있는지 먼저 확인한 다음, 'kubectl describe' 명령어로 해당 노드의 자세한
    정보를 확인한다.
    
        'kubectl get pods -o wide'
        'kubectl descrbie node [NODE....]'
        
    'kubectl describe node' 명령어의 출력 내용 중에서 중간 부분에 위치한 Non-terminated Pods 항목에서는 해당 노드에서 실행 중인 포드들의 자원
    할당량을 확인할 수 있다. 방금 생성한 resource-limit-pod라는 이름의 포드 외에도 kube-system 네임스페이스의 포드가 미리 존재하고 있을 것이다.
    이 포드들은 쿠버네티스의 네트워크를 위한 핵심 컴포넌트로, 따로 설정하지 않아도 기본적으로 CPU와 메모리가 할당된다.
    
    'kubectl describe node' 명령어의 출력 내용 중에서 중간 부분에 위치한 Non-terminated Pods 항목에서는 해당 노드에서 실행 중인 포드들의 자원
    할당량을 확인할 수 있다. 방금 생성한 resource-limit-pod라는 이름의 포드 외에도 kube-system 네임스페이스의 포드가 미리 존재하고 있을 것이다.
    이 포드들은 쿠버네티스의 네트워크를 위한 핵심 컴포넌트로, 따로 설정하지 않아도 기본적으로 CPU와 메모리가 할당된다.
    
    그 아래 Allocated resources 항목에서는 해당 노드에서 실행 중인 포드들의 자원 할당량을 모두 더한 값이 출력된다. 즉, 방금 생성한 resource-limit-pod
    라는 이름의 포드 및 다른 시스템 컴포넌트의 자원 할당량의 합계를 확인할 수 있다. 
    
    
    
            > 1.2 컨테이너와 포드의 자원 사용량 제한하기 : Request
    limits는 해당 포드의 컨테이너가 최대로 사용할 수 있는 자원의 상한선을 의미한다. 방금 생성한 포드 또한 YAML 파일에서 Limits를 설정했고, 포드의 컨테이너는
    Limits보다 더 많은 자원을 사용할 수 없다는 것을 알 수 있다. 여기서 Request라는 단어가 등장했다. 
    
    쿠버네티스의 자원 관리에서 Request는 '적어도 이 만큼의 자원은 컨테이너에게 보장돼야 한다.(하한선)'는 의미이다. Limits의 개념과 유사해보이지만, Requests는
    쿠버네티스에서 자원의 Overcommit을 가능하게 하는 개념이다. 
    Request가 정확히 어떤 기능을 하는지 설명하기 전에 자원의 오버커밋이 무엇인지 알아야 한다. 오버커밋은 한정된 컴퓨팅 자원을 효율적으로 사용하기 위한 방법으로,
    사용할 수 있는 자원보다 더 많은 양을 가상 머신이나 컨테이너에게 할당함으로써 전체 자원의 사용률(Utilization)을 높이는 방법이다. 
    
    1GB의 전체 메모리에서 A -> 512MB, B -> 512MB를 정적으로 할당하는 것은 비효율이 생긴다. 만약 사용률이 낮은 컨테이너가 높은 컨테이너에게 빌려줄 수 
    있다면 좋을 것이다. 이처럼 정적인 할당은 유휴 자원을 제대로 활용하지 못한다. 이러한 문제를 해결하는 가장 좋은 방법은 사용량을 예측하여 적절히 결정 하는
    것이지만, 컨테이너가 실제로 얼마나 사용할지 예측하기란 여간 어려운 일이다. 
    
    이를 위해서 쿠버네티스에서는 오버커밋을 통해 물리 자원보다 더 많은 양의 자원을 할 당할 수 있다. 1GB 전체 메모리라도 750MB, 750MB인 포드 두 개를 
    생성할 수 있다고 보면 된다. 실제 메모리가 1GB이므로 실제로는 그렇게 되지 않지만 A가 사용률이 낮다면 B에 빌려주는 식으로 자원을 효율적으로 사용하게 할 수 
    있다. 이러한 자원 제한을 쿠버네티스에서는 Limits라고 한다. 위에서 작성한 resource-limit-pod의 resources가 이러한 설정이다.
    그렇지만 만약 A가 500MB를 사용할 때, B가 750MB를 사용하고자 하면 어떻게 될까? 이러한 상황을 방지하기 위해서 새로운 개념이 도입된다. 각 컨테이너가
    '사용을 보장받을 수 있는 경계선'을 정하는 것이다. A와 B 컨테이너에 하한선을 512MB로 잡았지만 Limits은 750MB로 같다. 두 컨테이너 모두 메모리 사용을
    보장받을 수 있는 경계선이 500MB이기 떄문에 500MB 이내에 메모리를 사용한다면 문제가 되지 않는다.
    
    그렇지만 컨테이너 A가 500MB를 사용하고 있을 때 컨테이너 B가 750MB를 사용하려면 B는 메모리 사용에 실패한다. 컨테이너 A는 메모리 사용을 보장받을 수 
    있는 경계선 내에 500MB를 사용하고 있는데, 컨테이너 B가 이를 침범하려 했기 때문이다. 이러한 경계선을 쿠버네티스에서 Request라고 한다. 즉, Request는
    컨테이너가 최소한으로 보장받아야 하는 자원의 양을 뜻한다. 
    
    포드에 설정될 Request 자원의 값은 Limit과 같이 YAML에서 지정할 수 있다.
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-limit-with-request-pod
  labels:
    name: resource-limit-with-request-pod
spec:
  containers:
    - name: nginx
      image: nginx:latest
      resources:
        limits:
          memory: "256Mi"
          cpu: "1000m"
        requests:
          memory: "128Mi"
          cpu: "500m"
```
```dockerfile
    먼저 이 YAML 파일에 정의된 포드 컨테이너의 메모리 할당량을 보자 requests에서 128Mi를 limit을 256Mi로 설정했기에 '하한선 128mb 유휴 자원 존재 시
    256mb 까지 설정한다. '는 의미를 갖는다. CPU 또한 같은 원리로 해석하면 '하한선 0.5CPU ~ 상한선 1CPU' 이다.
    단, requests는 컨테이너가 보장받아야 하는 최소한의 자원을 뜻하기 때문에 노드의 총 자원의 크기보다 더 많은 양을 할당할 수는 없다. 따라서 쿠버네티스
    스케쥴러는 포드의 request만큼 여유가 있는 노드를 선택해 포드를 생성한다. 즉, 포드를 할당할 떄 사용되는 자원 할당 기준은 Limit이 아닌 Request이다.
    Requests의 값을 낮게 설정해서 포드를 생성하면 포드가 스케쥴링 되어 노드에 할당될 확률은 높아지겠지만, 포드가 사용을 보장받을 수 있는 자원의 양은 적어질 것이다.
    
        {
            노드에 할당된 포드의 Limits 값의 합은 노드의 물리 자원 크기를 초과할 수도 있다. 
        } 
        
            
            
            > 1.3 CPU 자원 사용량의 제한 원리
            쿠버네티스에서 CPU Requests와 Limits
    앞서 설명한 것처럼 쿠버네티스에서는 CPU를 m(밀리코어) 단위로 제한하며, 1개의 CPU는 1000m에 해당한다. 따라서 서버에 2개의 CPU가 존재한다면 최대
    2000m 만큼의 CPU Requests를 포드의 컨테이너에 할당할 수 있다.
    
    기본적인 CPU 자원의 단위를 이해했다면 이번에는 포드의 CPU Request와 Limit가 실제로 어떻게 동작하는지 알아보자. 이전에 사용했던 포드의 YAML 파일에는
    CPU의 Limits와 Requests가 각각 설정돼 있다. 
```
resource-limit-with-request-pod.yaml
```yaml
...
  resources:
    limits:
      memory: "256Mi"
      cpu: "1000m"      #최대 1개 CPU만큼 사용할 수 있다.
    requests:
      memory: "128Mi"
      cpu: "500m"       #최소한 0.5개의 CPU만큼 사용을 보장받을 수 있다.
```
```dockerfile
    CPU의 Limits를 의미하는 resources.limit.cpu를 1000m 설정하면 포드의 컨테이너는 최대 1개 만큼의 CPU를 사용할 수 있다. 즉, 이는 아래의 도커
    명령과 같다. 이는 컨테이너가 사용 가능한 CPU의 최대한도를 나타내기 때문에 어렵지 않게 이해할 수 있다. 
        {
            'docker run --cpus 1 ...'
            'docker run --cpu-period 100000 --cpu-runtime 100000....'
        }
    그렇다면 resources.requests.cpu를 설정해 CPU가 보장받아야하는 최소한의 CPU 자원을 설정하면 컨테이너에서 어떻게 설정될까? 결론부터 말하면 CPU의
    Requests는 docker run의 '--cpu-shares' 옵션과 같다. '--cpu-shares'는 서버에 CPU가 실제로 몇 개가 있는지에 상관없이 '--cpu-shares'의 할당
    비율에 따라 컨테이너가 사용할 수 있는 CPU 자원이 결정되는 옵션이다. '--cpu-shares'가 설정된 여러 개의 컨테이너가 동시에 존재한다면 각 컨테이너의
    '--cpu-shares' 비율에 따라 CPU를 사용할 수 있다. 예를 들어 3개의 컨테이너에서 '--cpu-shares'를 각각 1024, 1024, 512로 설정했다면 CPU의 
    사용률이 100%가 되는 포화 상태에서는 각 컨테이너가 2:2:1의 비율으로 CPU를 사용할 수 있을 것이다. 단, '--cpu-shares'을 얼마나 설정했는지와는 상관없이
    시스템에 CPU의 유휴 자원이 존재할 때는 CPU를 전부 사용할 수 있다. 
    
    도커 명령어를 공부할 떄는 '--cpu-shares' 옵션이 쓸모없어 보였을 수도 있지만, 이 옵션을 '--cpus(Limits)'와 함께하면 CPU 자원에 오버 커밋을 적용
    할 수 있다. 다시 쿠버네티스로 돌아와서 이번에는 '--cpu-shares(Requests)'와 '--cpus(Limits)'가 함께 설정된 컨테이너를 생각해보자
    
        {
            쿠버네티스는 CPU 단위를 밀리코어(m) 단위로 나타내기 때문에 Requests('--cpu-shares')의 값 또한 m으로 나타낸다. 이 값이 실제로 CPU
            Share의 값으로 변환되는 식은 아래와 같다.
                (Requests에 설정된 CPU 밀리 코어 값 * 1024) / 1000 = CPU Share의 값
                ex) (500m * 1024) / 1000 = 512 (실제로 컨테이너에 설정된 '--cpu-shares의 값')
        }
    
    컨테이너가 하나만 존재하는 상황이라면 Requests('--cpu-shares')와 상관없이 CPU의 Limits의 값(700m)만큼 사용할 수 있을 것이다. 그렇지만
    두 개의 컨테이너 Limit은 700m, requests는 500m이라면 어떨까?
    
    
```